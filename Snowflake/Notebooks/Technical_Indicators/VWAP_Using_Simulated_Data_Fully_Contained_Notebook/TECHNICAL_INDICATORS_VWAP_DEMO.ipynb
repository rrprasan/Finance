{
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "45tbbkbfnaxta5dqb6aq",
   "authorId": "8795186554644",
   "authorName": "PRAJAGOPAL",
   "authorEmail": "prasanna.rajagopal@snowflake.com",
   "sessionId": "081e7711-6aa0-4d03-84c0-68c8ec3f5340",
   "lastEditTime": 1755708934947
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9436c337-2035-4cbf-b1e9-a6ef2158aab2",
   "metadata": {
    "name": "MD_Prerequisite_",
    "collapsed": false
   },
   "source": "## Pre-requisite Python Packages\nPlease add these using the \"Packages\" drop-down:\n  - plotly=6.0.1\n  - snowflake=1.6.0\n  - snowflake-snowpark-python=1.35.0\n  - snowflake.core=1.6.0"
  },
  {
   "cell_type": "markdown",
   "id": "740896ce-8cbd-4aa1-bf67-77881d6fd276",
   "metadata": {
    "name": "MDUsingTechnicalIndicators",
    "collapsed": false
   },
   "source": "## Using Technical Indicators for Historical Analysis\n- ### Validate Investment Theses: Use past data to confirm if a trading strategy would have worked.\n- ### Identify Historical Trends: Discover long-term patterns of accumulation or distribution.\n- ### Analyze Market Conviction: Understand the strength behind historical price movements.\n- ### Optimize Portfolio Decisions: Refine entry and exit points for future trades based on past performance."
  },
  {
   "cell_type": "markdown",
   "id": "69c2b08b-e47b-47e6-8d9a-aa20d9f6cd36",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "### The Challenge: Scaling Technical Analysis\n\nWhile essential, calculating dozens of indicators across years of historical data presents significant data engineering challenges that can slow down or block analysis entirely.\n\n**Typical Hurdles:**\n* **Massive Data Wrangling:** Sourcing, cleaning, and aligning terabytes of granular trade and quote data is a complex and time-consuming first step.\n* **Complex Coding & Validation:** Each indicator‚Äîlike RSI, MACD, or Bollinger Bands‚Äîhas a unique mathematical formula that must be coded, tested, and validated.\n* **Intensive Computation:** The window functions and aggregations required are computationally expensive, often leading to slow performance on traditional systems.\n* **Brittle Pipelines:** Managing complex ETL jobs to chain these calculations together is an enormous engineering burden. If one step fails, the entire analysis breaks.\n\n**The Snowflake Solution:**\n* **Simplified Pipelines with Dynamic Tables:** We can replace complex, procedural ETL with simple, declarative SQL, letting Snowflake automate the dependencies and refreshes.\n* **Powerful SQL Engine:** Snowflake's engine is built for the scale and complexity of financial analysis, handling massive window functions and aggregations with ease.\n* **Separation of Compute & Storage:** We can instantly scale a warehouse up for heavy calculations and then scale it down, paying only for the compute we use.\n* **Snowpark for Python:** For highly specialized indicators, we can bring Python's rich libraries directly to the data, avoiding data movement."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "MD_Technical_Indicators_With_Snowflake_Dynamic_Tables"
   },
   "source": [
    "# Technical Indicators with Snowflake Dynamic Tables\n",
    "## Volume Weighted Average Price (VWAP) Analysis\n",
    "\n",
    "This notebook demonstrates how to calculate technical indicators using **Snowflake's Dynamic Tables** feature. We'll focus on Volume Weighted Average Price (VWAP) calculation using realistic stock market data.\n",
    "\n",
    "### Key Features:\n",
    "- **Realistic Market Data**: Generated using actual market patterns from AAPL and other major stocks\n",
    "- **Dynamic Tables**: Automated refresh with incremental processing\n",
    "- **Time Series Analysis**: 20-minute time slices and cumulative calculations\n",
    "- **Interactive Visualization**: Streamlit charts comparing intermediate vs final VWAP\n",
    "\n",
    "### Architecture Overview:\n",
    "1. **Raw Data Ingestion**: JSON market data simulation\n",
    "2. **Data Normalization**: Flatten JSON into structured records\n",
    "3. **Time Slice Aggregation**: 20-minute VWAP calculations\n",
    "4. **Cumulative Analysis**: Running VWAP using window functions\n",
    "5. **Visualization**: Interactive Streamlit dashboard\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "id": "6ed70050-1652-4f12-80a6-d19c5af42d17",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## What Are Dynamic Tables? The Future of Data Pipelines\n\nThink of Dynamic Tables as **automated and declarative data pipelines** built directly into Snowflake. Instead of writing complex, multi-step procedures with streams and tasks, you simply declare the final state of your data using a single `SELECT` statement.\n\n### Key Benefits for This Demo:\n\n* **Declarative & Simple**: You define **what** you want the final data to look like, and Snowflake handles **how** to get it there. Notice we don't write any procedural code to manage refreshes.\n\n* **Automated Orchestration**: Snowflake automatically builds and manages the entire dependency graph (DAG) between tables. When raw data is updated, Snowflake refreshes every downstream table in the correct order.\n\n* **Controlled Data Freshness**: You can easily define how up-to-date your data needs to be using the `TARGET_LAG` parameter, from near real-time to hours or days.\n\n* **Efficient & Cost-Effective**: Refreshes are incremental by default, only processing new or changed data. This runs on serverless compute, optimizing performance and cost."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "Set up the Snowflake environment with database, schema, and warehouse.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell3",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Environment Setup\n",
    "USE DATABASE DEMODB;\n",
    "USE SCHEMA EQUITY_RESEARCH;\n",
    "USE WAREHOUSE DEMO_XSMALL_WH;\n",
    "\n",
    "-- Verify connection\n",
    "SELECT CURRENT_DATABASE(), CURRENT_SCHEMA(), CURRENT_WAREHOUSE();\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell4"
   },
   "source": [
    "## Step 2: Create Raw Data Ingestion Table\n",
    "\n",
    "This transient table simulates the landing zone for JSON market data from external sources.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell5",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Create raw data ingestion table\nCREATE OR REPLACE TRANSIENT TABLE DEMODB.EQUITY_RESEARCH.DEMO_MARKET_DATA_JSON_INGESTION_TTBL (\n    TICKER VARIANT,\n    RESULTS VARIANT\n)\nCOMMENT = 'Raw JSON market data ingestion table - simulates data loading from an external stage using Snowpipe';\n",
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "MD_Step3_Insert_Realistic_AAPL_Data",
    "collapsed": false
   },
   "source": "## Step 3: Create Python Stored Procedure for Realistic AAPL Data\n\nLet's insert simulated trade data for 9 popular stocks. Each record contains:\n- **c**: Close price\n- **h**: High price  \n- **l**: Low price\n- **n**: Number of trades\n- **o**: Open price\n- **t**: Timestamp (Unix milliseconds)\n- **v**: Volume\n- **vw**: Volume weighted price\n\nThis Python stored procedure generates realistic AAPL stock data using advanced statistical modeling with numpy. It creates data that closely mimics real market patterns including:\n\n- **Geometric Brownian Motion**: For realistic price movements\n- **Intraday Volume Patterns**: Higher volume at open/close, lower at midday\n- **Proper OHLC Relationships**: Mathematically consistent price data\n- **Market Hours Only**: Trading data during 9:30 AM - 4:00 PM EST, weekdays only\n",
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "markdown",
   "id": "2165a640-1c02-4a03-857d-a7dea9156035",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": "## Format of the JSON Data Loaded into the raw table  \n```\n[\n  {\n    \"c\": 144.01,\n    \"h\": 144.29,\n    \"l\": 144.01,\n    \"n\": 68,\n    \"o\": 144.29,\n    \"t\": 1675242000000,\n    \"v\": 2665,\n    \"vw\": 144.1408\n  },\n  {\n    \"c\": 144.09,\n    \"h\": 144.1,\n    \"l\": 144.09,\n    \"n\": 36,\n    \"o\": 144.09,\n    \"t\": 1675242060000,\n    \"v\": 1174,\n    \"vw\": 144.0971\n  }\n]"
  },
  {
   "cell_type": "markdown",
   "id": "93f60190-d85e-4d07-ac4e-b1c76d72d4b5",
   "metadata": {
    "name": "MD_Generate_Stock_Trades",
    "collapsed": false
   },
   "source": "## Generate Realistic Stock Trades Data"
  },
  {
   "cell_type": "code",
   "id": "d7b66164-3983-40ef-8554-50ff18e26546",
   "metadata": {
    "language": "sql",
    "name": "SQLStoredProc_GenerateStockTradesData"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE PROCEDURE DEMODB.EQUITY_RESEARCH.GENERATE_AAPL_DATA_SIMULATION_PY_FINAL(\n    START_DATE_STR VARCHAR,\n    END_DATE_STR VARCHAR\n)\nRETURNS STRING\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.11'\nPACKAGES = ('snowflake-snowpark-python', 'numpy')\nHANDLER = 'run'\nAS\n$$\nimport json\nfrom datetime import datetime, timedelta\nimport numpy as np\n\ndef run(session, start_date_str, end_date_str):\n    # ===================================================================\n    # 1. Configuration Parameters\n    #    These variables control the simulation's behavior and are based\n    #    on realistic historical data for a stock like AAPL.\n    # ===================================================================\n    start_price = 130.0         # The price at which the simulation begins.\n    end_price = 230.0           # The target price at the end of the simulation period.\n    daily_volatility = 0.025    # The expected daily price fluctuation (e.g., 2.5%).\n    base_volume_per_min = 150000 # The average trading volume per minute.\n    ticker = 'AAPL'             # The stock ticker symbol.\n\n    # Convert the input strings to datetime objects for calculations.\n    start_date = datetime.strptime(start_date_str, '%Y-%m-%d').date()\n    end_date = datetime.strptime(end_date_str, '%Y-%m-%d').date()\n\n    # ===================================================================\n    # 2. Procedural Loop in Python to Generate Data Points\n    #    This section generates the minute-by-minute trade data.\n    # ===================================================================\n    all_datapoints = []         # An empty list to store the generated trade data.\n    current_date = start_date   # The date for the current iteration of the loop.\n    last_close_price = start_price # The closing price of the previous minute.\n\n    # Pre-calculate the drift and volatility to use in the loop.\n    total_days = (end_date - start_date).days\n    minute_drift = (pow(end_price / start_price, 1 / max(total_days, 1)) - 1) / 390\n    minute_volatility = daily_volatility / np.sqrt(390)\n\n    # Loop through each day from the start date to the end date.\n    while current_date <= end_date:\n        # Check if the current day is a weekday (Monday=0, ..., Sunday=6).\n        if current_date.weekday() < 5:\n            # Set the starting time for the trading day.\n            trading_time = datetime(current_date.year, current_date.month, current_date.day, 9, 30)\n            \n            # Loop 390 times to generate data for each minute of the trading day.\n            for i in range(390):\n                # Calculate the logarithmic return using the GBM formula.\n                log_return = (minute_drift - (minute_volatility**2 / 2)) + (minute_volatility * np.random.normal(0, 1))\n                # Calculate the new closing price based on the previous price and the log return.\n                close_price = last_close_price * np.exp(log_return)\n                # The opening price for this minute is the closing price of the last minute.\n                open_price = last_close_price\n                # Simulate the high and low prices for the minute.\n                high_price = max(open_price, close_price) + abs(np.random.normal(0, minute_volatility * close_price * 0.5))\n                low_price = min(open_price, close_price) - abs(np.random.normal(0, minute_volatility * close_price * 0.5))\n                \n                # Simulate a realistic \"U-shaped\" volume pattern (higher at open/close).\n                time_frac = i / 389.0 if 389 > 0 else 0\n                volume_multiplier = 1.0 + 1.5 * (1 - np.sin(np.pi * time_frac))\n                volume = int(base_volume_per_min * volume_multiplier * (0.75 + np.random.uniform(0, 0.5)))\n                \n                # Append the newly created data point to our list.\n                all_datapoints.append({\n                    \"o\": round(open_price, 2), \"h\": round(high_price, 2), \"l\": round(low_price, 2),\n                    \"c\": round(close_price, 2), \"v\": volume, \"t\": int(trading_time.timestamp() * 1000),\n                    \"vw\": round((high_price + low_price + close_price) / 3, 4)\n                })\n                \n                # Update the last closing price for the next iteration.\n                last_close_price = close_price\n                # Move to the next minute.\n                trading_time += timedelta(minutes=1)\n        \n        # Move to the next day.\n        current_date += timedelta(days=1)\n\n    # If no data was generated (e.g., only weekends in the date range), exit early.\n    if not all_datapoints:\n        return \"No trading days in the specified range. No data inserted.\"\n\n    # ===================================================================\n    # 3. Insert Data Using Optimized SQL\n    #    This section takes the generated data and inserts it into the Snowflake table.\n    # ===================================================================\n    \n    # Convert the list of Python dictionaries into a single JSON string.\n    json_string = json.dumps(all_datapoints)\n    # Escape any single quotes in the JSON string to prevent SQL errors.\n    escaped_json_string = json_string.replace(\"'\", \"''\")\n\n    # Create the final SQL INSERT statement.\n    sql = f\"\"\"\n    INSERT INTO DEMODB.EQUITY_RESEARCH.DEMO_MARKET_DATA_JSON_INGESTION_TTBL (TICKER, RESULTS)\n    SELECT\n        TO_VARIANT('{ticker}'),\n        PARSE_JSON('{escaped_json_string}')\n    \"\"\"\n    \n    # Execute the SQL statement.\n    session.sql(sql).collect()\n    # Return a success message with the total number of minutes processed.\n    return f\"Success! Processed and inserted {len(all_datapoints)} minutes of data for AAPL via Python procedure.\"\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "## Step 4: Check Existing Data Before Adding More\n\n**Important**: Before calling the data generation procedure, always check what date ranges already exist to _**avoid overlapping data.**_\n",
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell9",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Check existing data date ranges for AAPL\nSELECT \n    TICKER::STRING as TICKER_SYMBOL,\n    COUNT(*) as NUM_RECORDS,\n    SUM(ARRAY_SIZE(RESULTS)) as TOTAL_DATA_POINTS,\n    TO_DATE(TO_TIMESTAMP(MIN(RESULTS[0]:t::BIGINT) / 1000)) as FIRST_DATE,\n    TO_DATE(TO_TIMESTAMP(MAX(RESULTS[ARRAY_SIZE(RESULTS)-1]:t::BIGINT) / 1000)) as LAST_DATE,\n    DATEDIFF('day', \n        TO_DATE(TO_TIMESTAMP(MIN(RESULTS[0]:t::BIGINT) / 1000)),\n        TO_DATE(TO_TIMESTAMP(MAX(RESULTS[ARRAY_SIZE(RESULTS)-1]:t::BIGINT) / 1000))\n    ) + 1 as TOTAL_CALENDAR_DAYS\nFROM DEMO_MARKET_DATA_JSON_INGESTION_TTBL \nWHERE TICKER::STRING = 'AAPL'\nGROUP BY TICKER::STRING;\n",
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "## Step 5: Generate Initial AAPL Historical Data\n",
    "\n",
    "**‚ö†Ô∏è Date Range Guidelines:**\n",
    "- If table is empty: Start with any date range (e.g., '2023-01-01' to '2023-12-31')\n",
    "- If data exists from '2023-01-01' to '2023-12-31': Use '2024-01-01' to '2024-12-31' for next batch\n",
    "- **Always avoid overlapping dates** to prevent duplicate data\n",
    "\n",
    "Run this cell to generate your first batch of AAPL data:\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell11",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Generate initial AAPL data for 2023\n",
    "-- ‚ö†Ô∏è IMPORTANT: Adjust dates based on existing data check above!\n",
    "CALL DEMODB.EQUITY_RESEARCH.GENERATE_AAPL_DATA_SIMULATION_PY_FINAL('2023-01-01', '2023-12-31');\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "## Step 6: Create Dynamic Tables for VWAP Analysis\n\nNow we'll create the Dynamic Tables that automatically process our raw data into VWAP calculations.\n",
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "MD_CreateNormalizedTradeRecords_Dynamic_Table",
    "collapsed": false
   },
   "source": "## Step 6.1: Create Trade Records Normalization Dynamic Table\n\nThis Dynamic Table flattens the JSON data into normalized individual trade records. It extracts each trade from the RESULTS array and converts timestamps to readable dates.\n",
   "id": "ce110000-1111-2222-3333-ffffff000030"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "MD_CreateNormalizedTradeRecordsUsing_Dynamic_Table",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Create trade records normalization dynamic table\n",
    "CREATE OR REPLACE DYNAMIC TABLE DEMODB.EQUITY_RESEARCH.DEMO_TRADE_RECORDS_NORMALIZED_DTBL(\n",
    "    TICKER_SYMBOL,\n",
    "    TRADE_TIME,\n",
    "    TRADE_OPEN,\n",
    "    TRADE_HIGH,\n",
    "    TRADE_LOW,\n",
    "    TRADE_CLOSE,\n",
    "    TRADE_VOLUME,\n",
    "    TRADE_COUNT,\n",
    "    TRADE_VWAP\n",
    ")\n",
    "TARGET_LAG = 'DOWNSTREAM'\n",
    "REFRESH_MODE = INCREMENTAL\n",
    "INITIALIZE = ON_CREATE\n",
    "WAREHOUSE = DEMO_XSMALL_WH\n",
    "COMMENT = 'Normalized trade records from JSON data - foundation for technical indicators'\n",
    "AS\n",
    "SELECT \n",
    "    raw_data.TICKER::STRING AS TICKER_SYMBOL,\n",
    "    TO_TIMESTAMP(trade_record.value:t::BIGINT / 1000) AS TRADE_TIME,\n",
    "    trade_record.value:o::FLOAT AS TRADE_OPEN,\n",
    "    trade_record.value:h::FLOAT AS TRADE_HIGH,\n",
    "    trade_record.value:l::FLOAT AS TRADE_LOW,\n",
    "    trade_record.value:c::FLOAT AS TRADE_CLOSE,\n",
    "    trade_record.value:v::INTEGER AS TRADE_VOLUME,\n",
    "    trade_record.value:n::INTEGER AS TRADE_COUNT,\n",
    "    trade_record.value:vw::FLOAT AS TRADE_VWAP\n",
    "FROM DEMO_MARKET_DATA_JSON_INGESTION_TTBL raw_data,\n",
    "     LATERAL FLATTEN(input => raw_data.RESULTS) trade_record\n",
    "WHERE raw_data.TICKER::STRING = 'AAPL'  -- Focus on AAPL only\n",
    "ORDER BY TICKER_SYMBOL, TRADE_TIME ASC;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell33",
    "collapsed": false
   },
   "source": "## Step 6.2: Create VWAP 20-Minute Time Slices Dynamic Table\n\nThis Dynamic Table calculates VWAP values using 20-minute time slices. It aggregates trades within each time window and calculates the volume-weighted average price for intermediate analysis.\n",
   "id": "ce110000-1111-2222-3333-ffffff000032"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "SQL_20MIN_Time_Slice_DT",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Create 20-minute time slices dynamic table for intermediate VWAP\n",
    "CREATE OR REPLACE DYNAMIC TABLE DEMODB.EQUITY_RESEARCH.DEMO_VWAP_20MIN_TIME_SLICES_DTBL(\n",
    "    TRADE_TIME_SLICE,\n",
    "    TICKER_SYMBOL,\n",
    "    TICKER_SYMBOL_TRADE_TIME_SLICE,\n",
    "    SUM_PRICE,\n",
    "    SUM_VOLUME,\n",
    "    INTERMEDIATE_SUM_PRICE_VOLUME,\n",
    "    INTERMEDIATE_VWAP\n",
    ")\n",
    "TARGET_LAG = 'DOWNSTREAM'\n",
    "REFRESH_MODE = INCREMENTAL\n",
    "INITIALIZE = ON_CREATE\n",
    "WAREHOUSE = DEMO_XSMALL_WH\n",
    "COMMENT = '20-minute aggregated VWAP calculations for intermediate analysis'\n",
    "AS\n",
    "SELECT \n",
    "    TIME_SLICE(TRADE_TIME, 20, 'MINUTE') TRADE_TIME_SLICE,\n",
    "    TICKER_SYMBOL,\n",
    "    TICKER_SYMBOL || TO_VARCHAR(TIME_SLICE(TRADE_TIME, 20, 'MINUTE')) TICKER_SYMBOL_TRADE_TIME_SLICE,\n",
    "    SUM(TRADE_CLOSE) SUM_PRICE,\n",
    "    SUM(TRADE_VOLUME) SUM_VOLUME,\n",
    "    SUM(TRADE_CLOSE * TRADE_VOLUME) INTERMEDIATE_SUM_PRICE_VOLUME,\n",
    "    SUM(TRADE_CLOSE * TRADE_VOLUME) / SUM(TRADE_VOLUME) INTERMEDIATE_VWAP\n",
    "FROM DEMO_TRADE_RECORDS_NORMALIZED_DTBL\n",
    "GROUP BY TICKER_SYMBOL, TRADE_TIME_SLICE, TICKER_SYMBOL_TRADE_TIME_SLICE\n",
    "ORDER BY TICKER_SYMBOL, TRADE_TIME_SLICE ASC;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "MD_CreateCumulativeVWAP_Dynamic_Table",
    "collapsed": false
   },
   "source": "## Step 6.3: Create VWAP Cumulative Analysis Dynamic Table\n\nThis Dynamic Table calculates the final cumulative VWAP using window functions. It maintains running totals of price√óvolume and volume to calculate the true VWAP over time for each stock.\n",
   "id": "ce110000-1111-2222-3333-ffffff000034"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "SQL_CreateCumulativeVWAP_Dynamic_Table",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Create cumulative VWAP analysis dynamic table\n",
    "CREATE OR REPLACE DYNAMIC TABLE DEMODB.EQUITY_RESEARCH.DEMO_VWAP_CUMULATIVE_ANALYSIS_DTBL(\n",
    "    TRADE_TIME_SLICE,\n",
    "    TICKER_SYMBOL,\n",
    "    TICKER_SYMBOL_TRADE_TIME_SLICE,\n",
    "    CUMULATIVE_PRICE,\n",
    "    CUMULATIVE_VOLUME,\n",
    "    FINAL_VWAP\n",
    ")\n",
    "TARGET_LAG = '8 hours'\n",
    "REFRESH_MODE = INCREMENTAL\n",
    "INITIALIZE = ON_CREATE\n",
    "WAREHOUSE = DEMO_XSMALL_WH\n",
    "COMMENT = 'Final cumulative VWAP calculations using window functions'\n",
    "AS\n",
    "SELECT \n",
    "    TRADE_TIME_SLICE,\n",
    "    TICKER_SYMBOL,\n",
    "    TICKER_SYMBOL_TRADE_TIME_SLICE,\n",
    "    (SUM(SUM_PRICE) OVER (\n",
    "        PARTITION BY TICKER_SYMBOL \n",
    "        ORDER BY TRADE_TIME_SLICE ASC \n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "    )) CUMULATIVE_PRICE,\n",
    "    (SUM(SUM_VOLUME) OVER (\n",
    "        PARTITION BY TICKER_SYMBOL \n",
    "        ORDER BY TRADE_TIME_SLICE ASC \n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "    )) CUMULATIVE_VOLUME,\n",
    "    (SUM(INTERMEDIATE_SUM_PRICE_VOLUME) OVER (\n",
    "        PARTITION BY TICKER_SYMBOL \n",
    "        ORDER BY TRADE_TIME_SLICE ASC \n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "    )) / (SUM(SUM_VOLUME) OVER (\n",
    "        PARTITION BY TICKER_SYMBOL \n",
    "        ORDER BY TRADE_TIME_SLICE ASC \n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "    )) FINAL_VWAP\n",
    "FROM DEMO_VWAP_20MIN_TIME_SLICES_DTBL\n",
    "ORDER BY TICKER_SYMBOL, TRADE_TIME_SLICE ASC;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "MD_Dynamic_Table_Refresh"
   },
   "source": [
    "## Step 7: Understanding Dynamic Table Refresh\n",
    "\n",
    "Dynamic Tables automatically refresh when their underlying data changes. Here's how to monitor and manually refresh them when needed:\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "SQL_Dynamic_Table_Refresh",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Check Dynamic Table basic information\nSHOW DYNAMIC TABLES LIKE 'DEMO_%' IN SCHEMA DEMODB.EQUITY_RESEARCH;\n\n-- View Dynamic Table configuration\nSELECT \n    \"name\",\n    \"created_on\",\n    \"target_lag\",\n    \"refresh_mode\",\n    \"warehouse\",\n    \"comment\"\nFROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))\nWHERE \"name\" LIKE 'DEMO_%'\nORDER BY \"name\";\n\n-- Check Dynamic Table refresh history and status\nSELECT \n    \"NAME\",\n    \"REFRESH_ACTION\",\n    \"REFRESH_TRIGGER\", \n    \"STATE\",\n    \"REFRESH_START_TIME\",\n    \"REFRESH_END_TIME\",\n    \"DATA_TIMESTAMP\"\nFROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(\n    name => 'DEMODB.EQUITY_RESEARCH.DEMO_TRADE_RECORDS_NORMALIZED_DTBL'\n))\nORDER BY \"REFRESH_END_TIME\" DESC\nLIMIT 5;\n",
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "markdown",
   "id": "4084d510-923b-4c1a-bd4e-8d316abbd8a0",
   "metadata": {
    "name": "MD_Manual_Refresh_Dynamic_Table",
    "collapsed": false
   },
   "source": "## Step 8: Manual Refresh of Dynamic Table"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "SQL_Manual_Refresh_Dynamic_Table",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Manual refresh of Dynamic Tables (if needed)\n",
    "-- Run these commands to force refresh the Dynamic Tables in dependency order:\n",
    "\n",
    "-- 1. First refresh the base normalization table\n",
    "ALTER DYNAMIC TABLE DEMO_TRADE_RECORDS_NORMALIZED_DTBL REFRESH;\n",
    "\n",
    "-- 2. Then refresh the 20-minute time slices table\n",
    "ALTER DYNAMIC TABLE DEMO_VWAP_20MIN_TIME_SLICES_DTBL REFRESH;\n",
    "\n",
    "-- 3. Finally refresh the cumulative analysis table\n",
    "ALTER DYNAMIC TABLE DEMO_VWAP_CUMULATIVE_ANALYSIS_DTBL REFRESH;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "MD_Add_More_Data",
    "collapsed": false
   },
   "source": "## Step 9: Add More Historical Data (Optional)\n\nTo add more data, first check the existing date ranges, then call the procedure with non-overlapping dates:\n\n**Example Scenarios:**\n- **Current data**: 2023-01-01 to 2023-12-31 ‚Üí **Add**: 2024-01-01 to 2024-12-31\n- **Current data**: 2023-06-01 to 2023-12-31 ‚Üí **Add**: 2023-01-01 to 2023-05-31 (backfill)\n- **Empty table** ‚Üí **Add**: Any date range you prefer\n### After adding the data you can manually refresh the Dynamic Table. \n### Just refresh the downstream Dynamic Table. In this case it is DEMO_VWAP_CUMULATIVE_ANALYSIS_DTBL\n",
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell20",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Add additional AAPL data (adjust dates based on existing data!)\n-- ‚ö†Ô∏è CRITICAL: Check Step 4 results first to avoid overlapping dates!\n\n-- Example: If you already have 2023 data, add 2024 data:\nCALL DEMODB.EQUITY_RESEARCH.GENERATE_AAPL_DATA_SIMULATION_PY_FINAL('2024-01-01', '2024-12-31');\n\n-- Example: If you need to backfill earlier data:\n-- CALL DEMODB.EQUITY_RESEARCH.GENERATE_AAPL_DATA_SIMULATION_PY_FINAL('2022-01-01', '2022-12-31');\n\n-- ‚ö†Ô∏è Uncomment and modify ONE of the above lines based on your data needs!\n",
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "MD_VerifyVWAP",
    "collapsed": false
   },
   "source": "## Step 10: Verify VWAP Calculations\n\nCompare intermediate vs final VWAP calculations and verify data quality.\n",
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "SQLVerifyVWAP",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Compare intermediate vs final VWAP for AAPL\n",
    "SELECT \n",
    "    i.TRADE_TIME_SLICE,\n",
    "    i.TICKER_SYMBOL,\n",
    "    ROUND(i.INTERMEDIATE_VWAP, 2) as INTERMEDIATE_VWAP,\n",
    "    ROUND(f.FINAL_VWAP, 2) as FINAL_VWAP,\n",
    "    ROUND(ABS(i.INTERMEDIATE_VWAP - f.FINAL_VWAP), 4) as VWAP_DIFFERENCE,\n",
    "    i.SUM_VOLUME as PERIOD_VOLUME,\n",
    "    f.CUMULATIVE_VOLUME\n",
    "FROM DEMO_VWAP_20MIN_TIME_SLICES_DTBL i\n",
    "JOIN DEMO_VWAP_CUMULATIVE_ANALYSIS_DTBL f\n",
    "    ON i.TICKER_SYMBOL_TRADE_TIME_SLICE = f.TICKER_SYMBOL_TRADE_TIME_SLICE\n",
    "WHERE i.TICKER_SYMBOL = 'AAPL'\n",
    "ORDER BY i.TRADE_TIME_SLICE ASC\n",
    "LIMIT 20;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000021"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell37",
    "collapsed": false
   },
   "source": "## Step 11: Interactive Visualization\n\nCreate an interactive Streamlit dashboard.\n",
   "id": "ce110000-1111-2222-3333-ffffff000036"
  },
  {
   "cell_type": "markdown",
   "id": "4da0dd2c-eeb7-482e-99d5-34f33a9cdac5",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "## Visualize the 20-Minute Time Slice Volume Weighted Average Price & the Cumulative Volume-Weighted Average Price.  "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell39",
    "language": "python"
   },
   "outputs": [],
   "source": "import streamlit as st\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nsession = get_active_session()\n# Get list of available stocks\nstocks_query = \"\"\"\nSELECT DISTINCT TICKER_SYMBOL \nFROM DEMO_VWAP_CUMULATIVE_ANALYSIS_DTBL \nORDER BY TICKER_SYMBOL\n\"\"\"\nstocks_df = session.sql(stocks_query).to_pandas()\navailable_stocks = stocks_df['TICKER_SYMBOL'].tolist()\n\n# Streamlit UI\nst.title('üìà Stock VWAP Analysis Dashboard')\nst.markdown('### Volume Weighted Average Price (VWAP) Visualization')\n\n# Stock selector\nselected_stock = st.selectbox(\n    'Select a Stock:', \n    available_stocks,\n    index=0\n)\n\n# Query data for selected stock\nvwap_query = f\"\"\"\nSELECT \n    i.TRADE_TIME_SLICE,\n    i.TICKER_SYMBOL,\n    i.SUM_VOLUME,\n    i.INTERMEDIATE_VWAP,\n    f.CUMULATIVE_VOLUME,\n    f.FINAL_VWAP\nFROM DEMO_VWAP_20MIN_TIME_SLICES_DTBL i\nJOIN DEMO_VWAP_CUMULATIVE_ANALYSIS_DTBL f \n    ON i.TICKER_SYMBOL = f.TICKER_SYMBOL \n    AND i.TRADE_TIME_SLICE = f.TRADE_TIME_SLICE\nWHERE i.TICKER_SYMBOL = '{selected_stock}'\nORDER BY i.TRADE_TIME_SLICE\n\"\"\"\n\ndf = session.sql(vwap_query).to_pandas()\n\nif not df.empty:\n    # Create subplot with secondary y-axis\n    fig = make_subplots(\n        rows=2, cols=1,\n        subplot_titles=('VWAP Comparison', 'Volume Analysis'),\n        specs=[[{\"secondary_y\": False}],\n               [{\"secondary_y\": True}]],\n        vertical_spacing=0.1\n    )\n    \n    # Add VWAP lines to first subplot\n    fig.add_trace(\n        go.Scatter(\n            x=df['TRADE_TIME_SLICE'],\n            y=df['INTERMEDIATE_VWAP'],\n            mode='lines+markers',\n            name='Intermediate VWAP',\n            line=dict(color='blue', width=2),\n            marker=dict(size=6)\n        ),\n        row=1, col=1\n    )\n    \n    fig.add_trace(\n        go.Scatter(\n            x=df['TRADE_TIME_SLICE'],\n            y=df['FINAL_VWAP'],\n            mode='lines+markers',\n            name='Final VWAP',\n            line=dict(color='red', width=2),\n            marker=dict(size=6)\n        ),\n        row=1, col=1\n    )\n    \n    # Add volume bars to second subplot\n    fig.add_trace(\n        go.Bar(\n            x=df['TRADE_TIME_SLICE'],\n            y=df['SUM_VOLUME'],\n            name='Period Volume',\n            marker_color='lightblue',\n            opacity=0.7\n        ),\n        row=2, col=1\n    )\n    \n    # Add cumulative volume line to second subplot\n    fig.add_trace(\n        go.Scatter(\n            x=df['TRADE_TIME_SLICE'],\n            y=df['CUMULATIVE_VOLUME'],\n            mode='lines+markers',\n            name='Cumulative Volume',\n            line=dict(color='green', width=2),\n            marker=dict(size=4),\n            yaxis='y4'\n        ),\n        row=2, col=1\n    )\n    \n    # Update layout\n    fig.update_layout(\n        title=f'{selected_stock} - VWAP Analysis',\n        height=600,\n        showlegend=True,\n        hovermode='x unified'\n    )\n    \n    # Update y-axis labels\n    fig.update_yaxes(title_text=\"VWAP Price ($)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Period Volume\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Cumulative Volume\", secondary_y=True, row=2, col=1)\n    \n    # Update x-axis labels\n    fig.update_xaxes(title_text=\"Time Slice\", row=2, col=1)\n    \n    st.plotly_chart(fig, use_container_width=True)\n    \n    # Display summary statistics\n    st.subheader(f'{selected_stock} VWAP Summary')\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\n            \"Final VWAP\", \n            f\"${df['FINAL_VWAP'].iloc[-1]:.4f}\"\n        )\n    \n    with col2:\n        st.metric(\n            \"Total Volume\", \n            f\"{df['CUMULATIVE_VOLUME'].iloc[-1]:,}\"\n        )\n    \n    with col3:\n        st.metric(\n            \"Price Range\", \n            f\"${df['FINAL_VWAP'].min():.2f} - ${df['FINAL_VWAP'].max():.2f}\"\n        )\n    \n    with col4:\n        st.metric(\n            \"Time Periods\", \n            f\"{len(df)}\"\n        )\n    \n    # Display raw data table\n    with st.expander(\"View Raw Data\"):\n        st.dataframe(\n            df.round(4),\n            use_container_width=True\n        )\n        \nelse:\n    st.error(f\"No data found for {selected_stock}\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000038"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell40"
   },
   "source": [],
   "id": "ce110000-1111-2222-3333-ffffff000039"
  }
 ]
}