{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "oteqz4s4kmng5t5obari",
   "authorId": "8795186554644",
   "authorName": "PRAJAGOPAL",
   "authorEmail": "prasanna.rajagopal@snowflake.com",
   "sessionId": "7fe4e3b6-dcf1-4016-a5bc-257d517e0cdb",
   "lastEditTime": 1744171917588
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af678f34-f843-49a1-8156-03f73106f7c7",
   "metadata": {
    "name": "MDNotebookImagesStg",
    "collapsed": false
   },
   "source": "## Optional Step:\n### If you wish to see the embedded slide images in the various cells, we need to create a stage and upload the images to that stage. \n### You can skip this step and the associated image cells.  \n## Create a NOTEBOOK_IMAGES_STG to store images to display in your notebook.\n```\nCREATE STAGE NOTEBOOK_IMAGES_STG\n\tDIRECTORY = ( ENABLE = true ) \n\tENCRYPTION = ( TYPE = 'SNOWFLAKE_SSE' );\n```\n### Copy the following image files into the stage:\n- Snowflake_DT_Usage_Stats.png\n- Snowflake_DT_For_DE.png\n- Snowflake_DT_Challenges_Benefits.png\n- The image zip file (Snowflake_Dynamic_Table_Images.zip) can be [found here](https://github.com/rrprasan/Finance/tree/main/Snowflake/Notebooks/Technical_Indicators/VWAP_Using_Dynamic_Tables).  "
  },
  {
   "cell_type": "code",
   "id": "5c5dbe9d-4191-462a-b9da-1402e67cfd5f",
   "metadata": {
    "language": "sql",
    "name": "CrNotebookImagesStg",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "CREATE STAGE NOTEBOOK_IMAGES_STG\n\tDIRECTORY = ( ENABLE = true ) \n\tENCRYPTION = ( TYPE = 'SNOWFLAKE_SSE' );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f5abaaff-b8ac-426b-a413-af1c4e358432",
   "metadata": {
    "name": "MDImportPythonPackages",
    "collapsed": false
   },
   "source": "## :truck: Import Python Packages and Get Active Session to Snowflake :snowflake:"
  },
  {
   "cell_type": "code",
   "id": "04cbb5a8-e00f-4a17-9520-760176e39ffd",
   "metadata": {
    "language": "python",
    "name": "PyImportStmtsCreateSession",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Snowpark Pandas API\nimport modin.pandas as spd\n# Import the Snowpark pandas plugin for modin\nimport streamlit as st\nimport matplotlib.pyplot as plt\nimport snowflake.snowpark.modin.plugin\n\nfrom snowflake.snowpark.context import get_active_session\n# Create a snowpark session\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "713e1316-115a-440e-b31b-ac8bda625bfd",
   "metadata": {
    "language": "python",
    "name": "PyShowImageDynamicTableStats",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "image = session.file.get_stream(\"@NOTEBOOK_IMAGES_STG/Snowflake_DT_Usage_Stats.png\" , decompress=False).read() \n# Display the image\nst.image(image)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a3149756-e139-4cb0-82b7-a4b7755e3bd2",
   "metadata": {
    "language": "python",
    "name": "PyShowImageDTForDE",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "image = session.file.get_stream(\"@NOTEBOOK_IMAGES_STG/Snowflake_DT_For_DE.png\" , decompress=False).read() \n# Display the image\nst.image(image)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "47fb0fdb-ff0b-4cd6-8416-3d6cd298a8bb",
   "metadata": {
    "language": "python",
    "name": "PyShowImageDTBenefits",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "image = session.file.get_stream(\"@NOTEBOOK_IMAGES_STG/Snowflake_DT_Challenges_Benefits.png\" , decompress=False).read() \n# Display the image\nst.image(image)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "464ac974-fbc2-49e8-b66f-d28cc49e8625",
   "metadata": {
    "name": "MDBiggestBenefitsOfDT",
    "collapsed": false
   },
   "source": "# :snowflake: Biggest Benefits of Snowflake Dynamic Tables :snowflake:\n## :snowflake: :point_right: Focus on insights, not data wrangling.\n## :racehorse: :fast_forward: Define insights, Snowflake Dynamic Tables handles the rest. :airplane:\n## :bulb: :chart_with_upwards_trend: Drive investment decisions faster with automated data. :gem: :dart:\n\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "b783722e-3484-4318-80f9-f80e3c37e650",
   "metadata": {
    "name": "MDDTKeyConcepts",
    "collapsed": false
   },
   "source": "## Key Concepts About Data Pipeline Architecture Using Dynamic Tables\n### 1. **Source data:** Data generated by real-world entities and collected in frontline systems. This data is then ingested into Snowflake via ETL processes.\n\n### 2. **Raw data:** After ingestion, the data is stored in Snowflake tables, where it’s transformed into a form more suitable for analysis.\n\n### 3. **Modeled data:** These transformations result in a set of models, which present familiar concepts to consumers for analysis.\n\nSource: [Dynamic Table Query Performance](https://docs.snowflake.com/en/user-guide/dynamic-tables-performance-queries)"
  },
  {
   "cell_type": "markdown",
   "id": "7745af83-db4c-449d-b390-38c5a385859d",
   "metadata": {
    "name": "MDVWAPOverview",
    "collapsed": false
   },
   "source": "## Volume Weighted Average Price Using :snowflake: Snowflake :snowflake: Dynamic Table\n#### Author: [Prasanna Rajagopal](https://www.linkedin.com/in/prasannarajagopal/)\nLast Updated: April, 2025\n- The **Volume Weighted Average Price (VWAP)** is a **technical indicator** that represents the average price of a security weighted by its trading volume over a specific period. \n- It shows the **average price** at which a stock has traded throughout the day, with **more weight given to prices with higher volume**.\n- In this Notebook, we will use Snowflake's Dynamic Tables to calculate the 20-minute VWAP price. \n    - This will be our first Dynamic Table for VWAP.\n    - We will call this intermediate VWAP.\n- We will use the intermediate VWAP to calculate the new VWAP that takes into account all the VWAP data available utp to that point. \n    - We will call it the final VWAP.  \n\n#### Section 1: Download and Load Source Data into Named Internal Stage in Snowflake.\n#### Section 2: Load Trade Data Into Raw Table in Snowflake\n#### Section 3: Creating Dynamic Tables\n#### Section 4: Build the Volume Weighted Average Price Chart\n#### Section 5: Managing Dynamic Tables\n#### Section 6: Monitoring Dynamic Tables\n#### Section 7: Cost Monitoring\n#### Section 8: Dynamic Table Best Practices\n"
  },
  {
   "cell_type": "code",
   "id": "46d4cadc-8678-4fff-a225-b7c4bf3a3a32",
   "metadata": {
    "language": "python",
    "name": "PyVWAPArchitecture"
   },
   "outputs": [],
   "source": "image = session.file.get_stream(\"@NOTEBOOK_IMAGES_STG/VWAP_DT_Streamlit_Snowflake_Notebook.png\" , decompress=False).read() \n# Display the image\nst.image(image)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cf7d753a-a012-4166-b2bc-b43c36be9192",
   "metadata": {
    "name": "MDVWAPDemoNamedStage",
    "collapsed": false
   },
   "source": "## Section 1: Download and Load Source Data into Named Internal Stage in Snowflake.\n## VWAP Demo: Set-up Named Stage for Equity Trade Data."
  },
  {
   "cell_type": "code",
   "id": "fd4ef1c5-bca7-4162-83a1-6975f36179e2",
   "metadata": {
    "language": "sql",
    "name": "SQLCreateStgDynamicTableDemoStg"
   },
   "outputs": [],
   "source": "CREATE STAGE DYNAMIC_TABLE_DEMO_STG \n\tDIRECTORY = ( ENABLE = true ) \n\tENCRYPTION = ( TYPE = 'SNOWFLAKE_SSE' ) \n\tCOMMENT = 'Stage for Stock Prices JSON Files To Calculate the VWAP Using Dynamic Tables.';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "18fb86e5-b751-44a5-a6fc-37b4e7095a3f",
   "metadata": {
    "name": "MDDownloadEquityTradeData",
    "collapsed": false
   },
   "source": "## You have three (3) options for creating the data for this demo. \n### **Option #1:**\n#### You can use your equity trade data for this demo. \n#### You will have to make appropriate changes to the code to ingest the trade data into Snowflake. \n### **Option #2:**\n#### Use Snowflake Marketplace Data Providers\n#### For example: [Xignite Equity & FX Price Data](https://app.snowflake.com/marketplace/listing/GZSNZ3Y8LK/xignite-equity-fx-price-data)\n### **Option #3:**\n#### The second option is to create an account with a data service such as [Polygon.io](https://polygon.io/)\n#### You can create a Python client using the Polygon API and download the datafiles:\n#### Here's the Python Code:\n\n```python\nfrom polygon import RESTClient\nPolygonRESTclient = RESTClient(api_key=\"<Your API Key Here>\")\ntrade_limit = 50000\naggs = PolygonRESTclient.get_aggs(<ticker_symbol>, 1, \"minute\", <from date>, <to date>, raw = 'true', limit = trade_limit)\naggs_json = json.loads(str(aggs.data, 'UTF-8'))\ndownloadLocation = f\"<Your File Download Location>/<Your File Name>.json\"\nf = open(downloadLocation, 'w' )\nf.write(str(aggs_json))\nf.close()\n```\n\n## Load the files into the stage: DYNAMIC_TABLE_DEMO_STG  "
  },
  {
   "cell_type": "markdown",
   "id": "4dc67c8f-daa0-4eae-a677-dca27a57a95e",
   "metadata": {
    "name": "MDSampleTradeData",
    "collapsed": false
   },
   "source": "## Sample 1-Minute Aggregated Equity Trade Data From [Polygon](Polygon.io) :chart_with_upwards_trend:\n```json\n{\n    'ticker': 'MSFT', \n    'queryCount': 15267, \n    'resultsCount': 15267, \n    'adjusted': True, \n    'results': [\n                    {\n                     'v': 211, \n                     'vw': 247.5947, \n                     'o': 247.61, \n                     'c': 247.61, \n                     'h': 247.61, \n                     'l': 247.61, \n                     't': 1675242000000, \n                     'n': 7\n                     }\n                ]\n}\n```"
  },
  {
   "cell_type": "code",
   "id": "482040eb-69f5-42f1-8caf-37849e73f568",
   "metadata": {
    "language": "sql",
    "name": "SQL_List_Dynamic_Table_Demo_Stg"
   },
   "outputs": [],
   "source": "LIST @DYNAMIC_TABLE_DEMO_STG;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6f1797d-d059-4572-acb0-c0f7c984cb53",
   "metadata": {
    "name": "MDCreateRawTable",
    "collapsed": false
   },
   "source": "## Section 2: Load Trade Data Into Raw Table in Snowflake\n## Create a Table To Store the Raw Data from the Source Files in the Stage."
  },
  {
   "cell_type": "code",
   "id": "836f9f06-1e9f-4782-8636-8f81d6b0065b",
   "metadata": {
    "language": "sql",
    "name": "SQLCreateRawTbl"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TRANSIENT TABLE COMPANY_STOCK_TRADES_RAW_DT_DEMO_TBL \n(\n    TICKER VARIANT,\n    RESULTS VARIANT\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d11cd663-38ad-4afa-8b1a-9d162d072cc5",
   "metadata": {
    "name": "MDCopyDataIntoRawTbl",
    "collapsed": false
   },
   "source": "## Copy the Source Data from the Staged JSON Files into the Raw Table"
  },
  {
   "cell_type": "code",
   "id": "ddbf27b1-5048-4902-9804-6a01c12ff4c9",
   "metadata": {
    "language": "sql",
    "name": "SQLCopyDataIntoRawTbl"
   },
   "outputs": [],
   "source": "COPY INTO COMPANY_STOCK_TRADES_RAW_DT_DEMO_TBL\nFROM @DEMODB.EQUITY_RESEARCH.DYNAMIC_TABLE_DEMO_STG\nMATCH_BY_COLUMN_NAME = CASE_INSENSITIVE\nFILE_FORMAT = (type = 'JSON' STRIP_OUTER_ARRAY = TRUE)\nFORCE = TRUE; ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2a13e509-bb26-4838-9df3-264c73ae1ef4",
   "metadata": {
    "name": "MDSelectOnRawData",
    "collapsed": false
   },
   "source": "## Test the Raw Data Table With a SELECT statement."
  },
  {
   "cell_type": "code",
   "id": "4b860536-a4e7-4924-bcf2-821a2acaf730",
   "metadata": {
    "language": "sql",
    "name": "SQLSelectRawData"
   },
   "outputs": [],
   "source": "SELECT * FROM COMPANY_STOCK_TRADES_RAW_DT_DEMO_TBL LIMIT 1;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "388f707b-ed89-431d-920b-0810e9fd509b",
   "metadata": {
    "name": "MDCreateDynamicTables",
    "collapsed": false
   },
   "source": "# Section 3: Creating Dynamic Tables\n## Business Goal: Create Intermediate & Final Volume Weighted Average Price (VWAP)"
  },
  {
   "cell_type": "markdown",
   "id": "d72bbac3-58fa-44cd-be31-74e678a912ac",
   "metadata": {
    "name": "MDEnableChangeTracking",
    "collapsed": false
   },
   "source": "## [Enable Change Tracking](https://docs.snowflake.com/en/user-guide/dynamic-tables-create#enable-change-tracking)\n## To create a Dynamic Table on the raw data, you need CHANGE_TRACKING turned on for the raw data table. \n- When creating a dynamic table with **incremental refresh mode**, if change tracking is not already enabled on the tables that it queries, **Snowflake automatically attempts to enable change tracking on them.** \n- In order to support incremental refreshes, change tracking must be enabled with non-zero time travel retention on all underlying objects used by a dynamic table. \n- As underlying database objects change, so does the dynamic table. If you recreate an object, you must re-enable change tracking."
  },
  {
   "cell_type": "code",
   "id": "d6608054-4fb7-49c3-a86a-69d745a382ac",
   "metadata": {
    "language": "sql",
    "name": "SQLShowTablesCheckChange_Tracking"
   },
   "outputs": [],
   "source": "SHOW TABLES LIKE 'COMPANY_STOCK_TRADES_RAW_DT_DEMO_TBL';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65c7782c-7342-45dc-9463-08f5eefb81db",
   "metadata": {
    "language": "sql",
    "name": "SQLEnableChangeTracking"
   },
   "outputs": [],
   "source": "ALTER TABLE COMPANY_STOCK_TRADES_RAW_DT_DEMO_TBL SET CHANGE_TRACKING = TRUE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe7be103-f94f-43ec-8281-0c559d7004fc",
   "metadata": {
    "language": "sql",
    "name": "SQLShowWarehouses",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "SHOW WAREHOUSES;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da81d3e3-ecbc-45f7-a6c2-4303ddebfe82",
   "metadata": {
    "name": "MDCreateDT",
    "collapsed": false
   },
   "source": "# Section 3.1: Dynamic Table To Store Parsed Raw Data"
  },
  {
   "cell_type": "code",
   "id": "2f21826c-7e2c-4b11-a1bf-1268a485b3f0",
   "metadata": {
    "language": "python",
    "name": "PyShowImageDTInterface",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "image = session.file.get_stream(\"@NOTEBOOK_IMAGES_STG/Snowflake_DT_Interface.png\" , decompress=False).read() \n# Display the image\nst.image(image)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e358d067-1ad7-402b-8bda-dcb5258a0b9b",
   "metadata": {
    "name": "MDDTParameters",
    "collapsed": false
   },
   "source": "## Dynamic Table Parameters\n### Required Parameters\n- Name for the Dynamic Table: ```<name>```\n- ```TARGET_LAG = { num { seconds | minutes | hours | days } | DOWNSTREAM }```\n- ```WAREHOUSE = <warehouse_name>```\n- ```AS <query>```\n### A Few Other Paramaters \n- ```REFRESH_MODE = { AUTO | FULL | INCREMENTAL }```\n    - Default: AUTO\n- ```INITIALIZE = { ON_CREATE | ON_SCHEDULE }``` \n    - Default: ON_CREATE\n- ```CLUSTER BY ( expr [ , expr , ... ] )```\n\n[More Parameters](https://docs.snowflake.com/en/sql-reference/sql/create-dynamic-table)"
  },
  {
   "cell_type": "markdown",
   "id": "35106487-8072-421d-b22c-1947f6fa33da",
   "metadata": {
    "name": "MDFullVSIncrementalRefresh",
    "collapsed": false
   },
   "source": "## Full vs Incremental Refresh \n## Full refresh mode performance\n- A full refresh **executes the query** and **overwrites** the dynamic tables with the results. \n- The content of a dynamic table is the same regardless of whether full or incremental refresh mode is chosen.\n- Full refresh is typically used for complex queries or workloads where incremental refresh is less efficient.\n- To optimize full refresh performance, treat it like any other Snowflake query. \n- The cost includes both executing the query and inserting the results, not just the query execution.\n\n[Full Refresh Mode](https://docs.snowflake.com/en/user-guide/dynamic-tables-performance-refresh-mode#full-refresh-mode-performance)\n\n## Incremental refresh mode performance\n- An **incremental refresh focuses on applying changes** since the last refresh, making it more efficient for large datasets with small updates. \n- The content of a dynamic table is the same regardless of the chosen refresh mode.\n- Incremental refresh can be more **resource-efficient** because it skips reprocessing unchanged data.\n\n[Incremental Refresh Mode](https://docs.snowflake.com/en/user-guide/dynamic-tables-performance-refresh-mode#incremental-refresh-mode-performance)\n\n## Understanding incremental refresh performance\n- In an incremental refresh, most of the effort usually goes into computing changes in the dynamic table.\n- Computing changes depends on the query and can be quite complex. \n- A common misunderstanding is that an incremental refresh only scans changes in the source tables, not the source tables themselves.\n- For example, imagine a query that does an **inner join between tables A and B**. \n    - If a row is inserted into table A, it must be joined with table B to compute changes in the query. \n    - This single row in A can join with many rows in B, which can mean there's a lot of work even if there are only a few changes in the sources.\n\n[Incremental refresh performance](https://docs.snowflake.com/en/user-guide/dynamic-tables-performance-refresh-mode#understanding-incremental-refresh-performance)"
  },
  {
   "cell_type": "markdown",
   "id": "b285f719-1041-4c30-9c7b-7f9491c00605",
   "metadata": {
    "name": "MDSupportedQueriesInIncrementalRefeesh",
    "collapsed": false
   },
   "source": "## Supported queries in incremental refresh\n```WITH```\n- Common table expressions (CTE) that use incremental refresh supported features in the subquery.\n\nExpressions in ```SELECT```\n- Expressions including those using deterministic built-in functions and immutable user-defined functions.\n\n```FROM```\n\n- Source tables, views, Snowflake-managed Apache Iceberg™ tables, and other dynamic tables. \n- Subqueries outside of FROM clauses (for example, WHERE EXISTS) are not supported.\n\n```OVER```\n\n- All window functions.\n\n```WHERE/HAVING/QUALIFY```\n\n- Filters with the same expressions that are valid in SELECT.\n\n```JOIN``` \n- Other expressions for joining tables\n- Supported join types for incremental refresh include inner joins, outer-equi joins, cross joins, and lateral flatten (only the non-static FLATTEN table function). \n- You can specify any number of tables in the join, and updates to all tables in the join are reflected in the results of the query.\n- Selecting the flatten SEQ column from a lateral flatten join is not supported for incremental refresh.\n\n```UNION ALL```\n- Supported with incremental refresh mode.\n\n```GROUP BY```\n- Supported with incremental refresh mode.\n\n[Supported queries in incremental refresh](https://docs.snowflake.com/en/user-guide/dynamic-tables-refresh#supported-queries-in-incremental-refresh)"
  },
  {
   "cell_type": "markdown",
   "id": "7e3d7996-3c9a-4480-84c4-d2f11ec8e931",
   "metadata": {
    "name": "MDParseRawDataDT",
    "collapsed": false
   },
   "source": "# Section 3.1: Dynamic Table 1\n## Parse the Raw Data \n## Dynamic Table Name: PARSE_STOCK_TRADES_DT_DEMO_DT"
  },
  {
   "cell_type": "code",
   "id": "3ae75f73-b7f3-4319-8608-8420dfd2cb69",
   "metadata": {
    "language": "sql",
    "name": "CREATE_DT_PARSE_STOCK_TRADES_DT_DEMO_DT",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TRANSIENT DYNAMIC TABLE PARSE_STOCK_TRADES_DT_DEMO_DT\n(\n    TICKER_SYMBOL   VARCHAR,\n    TRADE_TIME      TIMESTAMP_NTZ,\n    TRADE_PRICE     NUMBER(20, 4),\n    TRADE_VOLUME    NUMBER\n)\nTARGET_LAG = DOWNSTREAM\nWAREHOUSE = VWAP_DT_WH\nREFRESH_MODE = INCREMENTAL\nAS\nSELECT\n    TICKER::VARCHAR                                 TICKER_SYMBOL,\n    TO_TIMESTAMP_NTZ(TO_NUMBER(trades.VALUE:\"t\"),3) TRADE_TIME,\n    TO_NUMBER(trades.VALUE:\"c\",14, 4)               TRADE_PRICE,\n    TO_NUMBER(trades.VALUE:\"v\")                     TRADE_VOLUME\nFROM \n    COMPANY_STOCK_TRADES_RAW_DT_DEMO_TBL CSTR,\n    LATERAL FLATTEN (input => CSTR.RESULTS) TRADES\nORDER BY TICKER_SYMBOL, TRADE_TIME;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e4fd3195-9c79-4cb0-a2f7-6821b1bce2ab",
   "metadata": {
    "name": "MDVWAP20Minute",
    "collapsed": false
   },
   "source": "# Section 3.2: Dynamic Table 2\n## Calculate the Volume Weighted Average Price for 20-Minute Intervals."
  },
  {
   "cell_type": "code",
   "id": "471eaa59-2471-456a-8a60-aeac5b1230c9",
   "metadata": {
    "language": "sql",
    "name": "CREATE_DT_INTERMEDIATE_VWAP_STOCK_TRADES_DT_DEMO_DT",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TRANSIENT DYNAMIC TABLE INTERMEDIATE_VWAP_STOCK_TRADES_DT_DEMO_DT\n(\n    TRADE_TIME_SLICE                TIMESTAMP_NTZ,\n    TICKER_SYMBOL                   VARCHAR,\n    TICKER_SYMBOL_TRADE_TIME_SLICE  VARCHAR, \n    SUM_PRICE                       NUMBER(20, 4),\n    SUM_VOLUME                      NUMBER,\n    INTERMEDIATE_SUM_PRICE_VOLUME   NUMBER(20, 4),\n    INTERMEDIATE_VWAP               NUMBER(20, 4) \n)\nTARGET_LAG = DOWNSTREAM\nWAREHOUSE = VWAP_DT_WH\nREFRESH_MODE = INCREMENTAL\nAS\nSELECT\n    TIME_SLICE(TRADE_TIME, 20, 'MINUTE') TRADE_TIME_SLICE,\n    TICKER_SYMBOL,\n    TICKER_SYMBOL || TO_VARCHAR(TRADE_TIME_SLICE) TICKER_SYMBOL_TRADE_TIME_SLICE,\n    SUM(TRADE_PRICE)    SUM_PRICE,\n    SUM(TRADE_VOLUME)   SUM_VOLUME,\n    SUM(TRADE_PRICE * TRADE_VOLUME) INTERMEDIATE_SUM_PRICE_VOLUME,\n    SUM(TRADE_PRICE * TRADE_VOLUME)/SUM(TRADE_VOLUME)  INTERMEDIATE_VWAP\nFROM \n    PARSE_STOCK_TRADES_DT_DEMO_DT\nGROUP BY TICKER_SYMBOL, TRADE_TIME_SLICE, TICKER_SYMBOL_TRADE_TIME_SLICE\nORDER BY TICKER_SYMBOL, TRADE_TIME_SLICE ASC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c1fe5fa0-5497-4a6c-a718-a06deb7b279a",
   "metadata": {
    "language": "sql",
    "name": "SQLSelect20MinuteVWAP",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM INTERMEDIATE_VWAP_STOCK_TRADES_DT_DEMO_DT LIMIT 200;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7daa2eee-6b55-446e-abea-c9de2caf444e",
   "metadata": {
    "name": "MDCumulativeVWAP",
    "collapsed": false
   },
   "source": "# Section 3.3: Dynamic Table 3\n## Calculate the Cumulative Volume Weighted Average Price Based on Available Data in the Table."
  },
  {
   "cell_type": "code",
   "id": "c9d59ca1-93bc-4362-acc7-f21c6fcde357",
   "metadata": {
    "language": "sql",
    "name": "SQLCreateCumulativeVWAP",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TRANSIENT DYNAMIC TABLE VWAP_STOCK_TRADES_DT_DEMO_DT\n(\n    TRADE_TIME_SLICE                    TIMESTAMP_NTZ,\n    TICKER_SYMBOL                       VARCHAR,\n    TICKER_SYMBOL_TRADE_TIME_SLICE      VARCHAR, \n    CUMULATIVE_PRICE                    NUMBER(20,4),\n    CUMULATIVE_VOLUME                   NUMBER,\n    FINAL_VWAP                          NUMBER(20,4)\n)\nTARGET_LAG = '30 minutes'\nWAREHOUSE = VWAP_DT_WH\nREFRESH_MODE = INCREMENTAL\nAS\nSELECT\n    TRADE_TIME_SLICE,\n    TICKER_SYMBOL, \n    TICKER_SYMBOL_TRADE_TIME_SLICE,\n    (SUM(SUM_PRICE) OVER  (PARTITION BY TICKER_SYMBOL ORDER BY TRADE_TIME_SLICE ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) CUMULATIVE_PRICE,\n    (SUM(SUM_VOLUME) OVER (PARTITION BY TICKER_SYMBOL ORDER BY TRADE_TIME_SLICE ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) CUMULATIVE_VOLUME,\n    (SUM(INTERMEDIATE_SUM_PRICE_VOLUME) OVER (PARTITION BY TICKER_SYMBOL ORDER BY TRADE_TIME_SLICE ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW))/(SUM(SUM_VOLUME) OVER     (PARTITION BY TICKER_SYMBOL ORDER BY TRADE_TIME_SLICE ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) FINAL_VWAP\nFROM\n    INTERMEDIATE_VWAP_STOCK_TRADES_DT_DEMO_DT\nORDER BY TICKER_SYMBOL, TRADE_TIME_SLICE ASC;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2d9e34b2-d1fb-4ab7-840f-db0c2655b2c8",
   "metadata": {
    "name": "MDDTLimitationsSupportedDataTypes",
    "collapsed": false
   },
   "source": "## Limitations\n- You **can’t truncate** data from a dynamic table.\n- You **can’t create a temporary dynamic table**.\n\n[More Limitations](https://docs.snowflake.com/en/user-guide/dynamic-tables-limitations#general-limitations).\n## Supported Data Types\n- Dynamic tables support all Snowflake SQL data types for both incremental and full refresh.\n\n[Supported Data Types](https://docs.snowflake.com/en/user-guide/dynamic-tables-limitations#supported-data-types)\n\n**Except:**\n- **Structured data types.**\n    - Array, Object, Map.  \n- **Geospatial data types (full refresh only).**\n    - Geography, Geometry.\n## Limitation for Dynamic Table Query Constructs\n- External functions.\n- Sequences.\n- Functions that rely on CURRENT_USER. Dynamic table refreshes act as their owner role with a special SYSTEM user.\n- Sources that include directory tables, external tables, streams, and materialized views.\n- Views on dynamic tables or other unsupported objects.\n- User-defined table functions (UDTF) written in SQL.\n- User-defined functions (UDF) written in SQL that contain a subquery (for example, a SELECT statement).\n- Importing UDFs from an external stage.\n- PIVOT and UNPIVOT constructs are not supported in incremental or full refresh.\n- SAMPLE / TABLESAMPLE constructs are not supported in dynamic table incremental or full refresh.\n\n[Limitations on Query Constructs](https://docs.snowflake.com/en/user-guide/dynamic-tables-limitations#limitations-on-query-constructs)"
  },
  {
   "cell_type": "markdown",
   "id": "ed1a14b8-1f3a-46ed-8da5-9aefee2e5a03",
   "metadata": {
    "name": "MDBuildVWAPPyChart",
    "collapsed": false
   },
   "source": "# Section 4: Build the Volume Weighted Average Price Chart"
  },
  {
   "cell_type": "code",
   "id": "8a2b5324-4bd1-416d-827f-b7556599d02b",
   "metadata": {
    "language": "python",
    "name": "PySetSAVEDATAPATH"
   },
   "outputs": [],
   "source": "# Name of the sample database and the schema to be used\nSOURCE_DATA_PATH = \"DEMODB.EQUITY_RESEARCH\"\nSAVE_DATA_PATH = \"DEMODB.EQUITY_RESEARCH\"\n# Make sure we use the created database and schema for temp tables etc\nsession.use_schema(SAVE_DATA_PATH)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e3aeff07-6882-494c-9577-4af2ea7bcded",
   "metadata": {
    "language": "python",
    "name": "PyCreateVWAPChart"
   },
   "outputs": [],
   "source": "# --- Section 1: Set the Ticker for VWAP Retrieval From Dynamic Tables\ncompPickedForVWAP = 'MSFT'\n\n# # Display the user's selected company ticker symbol in the Streamlit application.\n\nst.write(compPickedForVWAP)\n\n# --- Section 2: Reading Intermediate VWAP Data from Snowflake ---\n# Define a variable (commented out) that seems to represent a list of columns\n# intended to be kept from an intermediate VWAP calculation. This variable is not used\n# in the subsequent code but might have been part of a previous or planned implementation.\n# intermediate_VWAP_keep_cols = ['TRADE_TIME_SLICE', 'L_LINENUMBER', 'L_PARTKEY', 'L_RETURNFLAG', 'L_QUANTITY', 'L_DISCOUNT', 'L_EXTENDEDPRICE']\n\n# Read data from a Snowflake table named 'INTERMEDIATE_VWAP_STOCK_TRADES_DT'.\n# The table name is constructed using the 'SOURCE_DATA_PATH' variable (defined elsewhere).\n# The data is read using a function 'spd.read_snowflake()', which utilizes the Snowflake connector.\n# The resulting DataFrame is then sorted by 'TICKER_SYMBOL' and 'TRADE_TIME_SLICE' in ascending order.\n\n\nintermediate_Ticker_VWAP_df = spd.read_snowflake(f\"{SOURCE_DATA_PATH}.INTERMEDIATE_VWAP_STOCK_TRADES_DT_DEMO_DT\").sort_values([\"TICKER_SYMBOL\",\"TRADE_TIME_SLICE\"], ascending = True)\n\n# Filter the 'intermediate_Ticker_VWAP_df' to include only rows where the 'TICKER_SYMBOL'\n# matches the company selected by the user ('compPickedForVWAP').\n# The '.where()' method in pandas returns a DataFrame with NaN values where the condition is False.\n\nfiltered_intermediate_Ticker_VWAP_df = intermediate_Ticker_VWAP_df.where(intermediate_Ticker_VWAP_df['TICKER_SYMBOL'] == compPickedForVWAP)\n\n# Remove rows containing NaN values from the filtered DataFrame. This effectively keeps only\n# the data corresponding to the selected company.\n\nfiltered_intermediate_Ticker_VWAP_df = filtered_intermediate_Ticker_VWAP_df.dropna()\n\n# --- Section 3: Reading Final VWAP Data from Snowflake ---\n# Similar to Section 5, read data from another Snowflake table named 'VWAP_STOCK_TRADES_DT'.\n# This table likely contains the final calculated VWAP values.\n\nfinal_Ticker_VWAP_df = spd.read_snowflake(f\"{SOURCE_DATA_PATH}.VWAP_STOCK_TRADES_DT_DEMO_DT\")\n\n# Filter the 'final_Ticker_VWAP_df' to include only rows where the 'TICKER_SYMBOL'\n# matches the company selected by the user.\n\nfiltered_final_Ticker_VWAP_df = final_Ticker_VWAP_df.where(final_Ticker_VWAP_df['TICKER_SYMBOL'] == compPickedForVWAP)\n\n# Remove rows containing NaN values from the filtered final VWAP DataFrame.\nfiltered_final_Ticker_VWAP_df = filtered_final_Ticker_VWAP_df.dropna()\n\n\n# --- Section 4: Merging Intermediate and Final VWAP Data ---\n# Merge the filtered intermediate and final VWAP DataFrames based on a common column\n# 'TICKER_SYMBOL_TRADE_TIME_SLICE'.\n# - 'left_on' and 'right_on' specify the columns to use for the merge from the left and right DataFrames, respectively.\n# - 'how='left'' indicates a left merge, meaning all rows from the 'filtered_intermediate_Ticker_VWAP_df'\n#   will be included, and matching rows from 'filtered_final_Ticker_VWAP_df' will be joined. If there's no match\n#   in the right DataFrame, the columns from the right DataFrame will have NaN values.\n\nspd_intermediate_and_final_Ticker_vwap_df = filtered_intermediate_Ticker_VWAP_df.merge(filtered_final_Ticker_VWAP_df,\n                                            left_on='TICKER_SYMBOL_TRADE_TIME_SLICE', \n                                            right_on='TICKER_SYMBOL_TRADE_TIME_SLICE', \n                                            how='left')\n\n# --- Section 5: Preparing Data for Plotting ---\n# Create a dictionary containing the data to be used for plotting.\n# It extracts the 'TRADE_TIME_SLICE' from the intermediate VWAP DataFrame\n# (assuming the trade time slices are consistent across both DataFrames and are represented by '_x' suffix after the merge),\n# the 'INTERMEDIATE_VWAP' from the intermediate VWAP DataFrame,\n# and the 'FINAL_VWAP' from the merged DataFrame (which originated from the final VWAP data).\n\ndata = {\n    'TRADE_TIME_SLICE_x': spd_intermediate_and_final_Ticker_vwap_df['TRADE_TIME_SLICE_x'],\n    'INTERMEDIATE_VWAP': spd_intermediate_and_final_Ticker_vwap_df['INTERMEDIATE_VWAP'],\n    'FINALVWAP': spd_intermediate_and_final_Ticker_vwap_df['FINAL_VWAP']\n}\n\n# Create a pandas DataFrame from the prepared data dictionary.\n\ndf = spd.DataFrame(data)\n\n# --- Section 6: Creating and Displaying the Plot ---\n# Create a new figure and a set of subplots using matplotlib.\n# 'figsize' sets the size of the plot.\n\nplt.figure(figsize=(15, 6))\n\n# Plot the 'INTERMEDIATE_VWAP' against 'TRADE_TIME_SLICE_x'.\n# 'label' is used for the legend.\n\nplt.plot(df['TRADE_TIME_SLICE_x'], df['INTERMEDIATE_VWAP'], label='INTERMEDIATE_VWAP')\n\n# Plot the 'FINAL_VWAP' against 'TRADE_TIME_SLICE_x'.\n\nplt.plot(df['TRADE_TIME_SLICE_x'], df['FINAL_VWAP'], label='FINAL_VWAP')\n\n# Add a title to the plot, including the name of the selected company.\n\nplt.title(f\"{compPickedForVWAP} - Volume Weighted Average Price (VWAP)\")\n\n# Add a label to the x-axis.\n\nplt.xlabel('TRADE')\n\n# Add a label to the y-axis.\n\nplt.ylabel('VWAP')\n\n# Display the legend, which identifies the plotted lines.\n\nplt.legend()\n\n# Show the created plot. This will display the visualization in the Streamlit application.\n\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2133ec3b-d71b-4951-a8eb-7a903a4d60b3",
   "metadata": {
    "name": "MDManagingDT",
    "collapsed": false
   },
   "source": "# Section 5: Managing Dynamic Tables\n```SQL \nSHOW DYNAMIC TABLES LIKE 'vwap%' IN SCHEMA DEMODB.EQUITY_RESEARCH;\n```\n\n```SQL\nDESC DYNAMIC TABLE INTERMEDIATE_VWAP_STOCK_TRADES_DT_DEMO_DT;\n```\n"
  },
  {
   "cell_type": "code",
   "id": "b94758cf-98d8-4619-836c-6860c6f4814b",
   "metadata": {
    "language": "sql",
    "name": "SQLShowDT"
   },
   "outputs": [],
   "source": "SHOW DYNAMIC TABLES LIKE '%vwap%'; -- IN SCHEMA mydb.myschema;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0358a2a1-503f-45af-8d83-ba8315c19fb8",
   "metadata": {
    "language": "sql",
    "name": "SQLShowDTInSchema"
   },
   "outputs": [],
   "source": "SHOW DYNAMIC TABLES LIKE '%vwap%' IN SCHEMA DEMODB.EQUITY_RESEARCH;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7d7ee68b-879b-4f18-a0b6-ee0494f6467f",
   "metadata": {
    "language": "sql",
    "name": "SQLDESCDT1"
   },
   "outputs": [],
   "source": "DESC DYNAMIC TABLE INTERMEDIATE_VWAP_STOCK_TRADES_DT_DEMO_DT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b6effc8-66f2-4c2b-9118-ee8d1e61e9f5",
   "metadata": {
    "language": "sql",
    "name": "SQLDESCDT2"
   },
   "outputs": [],
   "source": "DESC DYNAMIC TABLE VWAP_STOCK_TRADES_DT_DEMO_DT;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "823d661f-0e2b-437f-93a5-f0885777c0b1",
   "metadata": {
    "name": "MDChangeWarehouseDT",
    "collapsed": false
   },
   "source": "## Change the Warehouse for a Dynamic Table"
  },
  {
   "cell_type": "code",
   "id": "401aa84d-9cf8-4207-bc80-ded910748e10",
   "metadata": {
    "language": "sql",
    "name": "SQLAlterDTChangeWH1"
   },
   "outputs": [],
   "source": "-- Current WH: VWAP_DT_WH\n-- New WH: DEMO_XSMALL_WH\nALTER DYNAMIC TABLE VWAP_STOCK_TRADES_DT_DEMO_DT SET WAREHOUSE = DEMO_XSMALL_WH;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2cc58bf-0e5b-4c4c-99bb-5cf24b9a020f",
   "metadata": {
    "language": "sql",
    "name": "SQLAlterDTChangeWH2"
   },
   "outputs": [],
   "source": "ALTER DYNAMIC TABLE VWAP_STOCK_TRADES_DT_DEMO_DT SET WAREHOUSE = VWAP_DT_WH;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5b5ee658-6bfc-4be2-9c3c-ff314a9f6144",
   "metadata": {
    "name": "MDSwappingDT",
    "collapsed": false
   },
   "source": "## [Swapping Dynamic Tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-manage#swap-dynamic-tables)\n- Allows for a seamless transition between datasets or table versions without disrupting workflows or modifying dependent scripts. \n- If you’re developing a new version of a table but want to keep the same name for ongoing processes, swapping lets you replace the old table with the new one. \n- This approach ensures continuity while enabling updates, testing, or upgrades with minimal downtime or disruption."
  },
  {
   "cell_type": "code",
   "id": "603cc472-0ecf-4ce6-8486-d15eb681f3ec",
   "metadata": {
    "language": "sql",
    "name": "SQLSwappingDT"
   },
   "outputs": [],
   "source": "ALTER DYNAMIC TABLE my_dynamic_table SWAP WITH my_new_dynamic_table;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bb109bf8-cfb2-4d41-820d-97b9176d2e45",
   "metadata": {
    "name": "MDClustering",
    "collapsed": false
   },
   "source": "## [Cluster dynamic tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-manage#cluster-dynamic-tables)\n- Clustering dynamic tables can enhance performance by improving query efficiency and refresh operations:\n- **Query efficiency:** Clustering dynamic tables can help speed up queries, just like with regular tables, by clustering on common join keys or filter columns.\n- **Refresh operations:** Clustering can also help make refreshes faster if the clustering keys align with frequent change patterns\n- Clustering by user ID can be effective when you have updates where a handful of users change."
  },
  {
   "cell_type": "code",
   "id": "85820848-ac20-49d0-ae31-9b90a3924eef",
   "metadata": {
    "language": "sql",
    "name": "SQLClusterDTEg1"
   },
   "outputs": [],
   "source": "ALTER DYNAMIC TABLE INTERMEDIATE_VWAP_STOCK_TRADES_DT_DEMO_DT CLUSTER BY (TRADE_TIME_SLICE);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "be794833-6a83-400b-8efb-04af0ec8b221",
   "metadata": {
    "language": "sql",
    "name": "SQLClusterDTEg2"
   },
   "outputs": [],
   "source": "ALTER DYNAMIC TABLE VWAP_STOCK_TRADES_DT_DEMO_DT CLUSTER BY (TRADE_TIME_SLICE);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2545b021-3165-40e6-8bab-e5efc4a5ae00",
   "metadata": {
    "name": "MDDropDT",
    "collapsed": false
   },
   "source": "## [Drop Dynamic Table](https://docs.snowflake.com/en/user-guide/dynamic-tables-manage#drop-existing-dynamic-tables)"
  },
  {
   "cell_type": "code",
   "id": "27e1ba19-6845-4b6a-8bb1-747ef70ae0ce",
   "metadata": {
    "language": "sql",
    "name": "SQLDropDT"
   },
   "outputs": [],
   "source": "DROP DYNAMIC TABLE INTERMEDIATE_VWAP_STOCK_TRADES_DT_DEMO_DT;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "26e681a2-3f6c-4e8b-8431-97e8bafc80be",
   "metadata": {
    "name": "MDUndropDT",
    "collapsed": false
   },
   "source": "## [Restore Dynamic Table](https://docs.snowflake.com/en/user-guide/dynamic-tables-manage#restore-dropped-dynamic-tables)\n- Note that you can only undrop dynamic tables within the retention period (default is 24 hours).\n- You have the OWNERSHIP privilege on that dynamic table."
  },
  {
   "cell_type": "code",
   "id": "e29078eb-0a6a-4481-94ff-7a575a577eb1",
   "metadata": {
    "language": "sql",
    "name": "SQLRestoreDT"
   },
   "outputs": [],
   "source": "UNDROP DYNAMIC TABLE INTERMEDIATE_VWAP_STOCK_TRADES_DT_DEMO_DT;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "68721c52-708c-4aa0-adfc-f35efd9fab20",
   "metadata": {
    "name": "MDUnderstandingEffectsOfColumnChangesBaseTables",
    "collapsed": false
   },
   "source": "## [Understanding the effects of changes to columns in base tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-manage#understanding-the-effects-of-changes-to-columns-in-base-tables)\n- Change: **New column added or Existing unused column removed.**\n    - Impact: None. If a new column is added to the base table or an unused column is deleted, no action occurs and refreshes continue as before.\n- Change: **Underlying base table is recreated with identical column names and types**\n- Change: **Underlying base table column is recreated with the same name and type.**\n    - Impact: Full refresh/reinitialize: During the next refresh cycle, a full refresh is done to ensure that no incorrect or stale data is in the dynamic table.\n- Change: **An underlying column or other element used by a dynamic table changes in name or in some other way.**\n    - Impact: The state of the dynamic table changes to FAILED. The dynamic table must be recreated to respond to the change.\n"
  },
  {
   "cell_type": "markdown",
   "id": "b3c18c22-3f52-4463-9bc7-edf16dcd0d73",
   "metadata": {
    "name": "MDSuspendResumeDT",
    "collapsed": false
   },
   "source": "## [Suspend or resume dynamic tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-manage#suspend-or-resume-dynamic-tables)\n- Dynamic tables are automatically suspended after five consecutive scheduled refresh errors. \n- A successful refresh, including a manual refresh, resets the error count to zero. \n- For example, if a table fails two consecutive scheduled refreshes, then succeeds on the next, the error count resets to zero.\n- Errors from manually triggered refreshes don’t count toward this limit.\n- Any dynamic tables dependent on a suspended table are also suspended."
  },
  {
   "cell_type": "code",
   "id": "7efa6171-8678-4ff8-8d83-ec2d44d5929f",
   "metadata": {
    "language": "sql",
    "name": "SQLSuspendDT"
   },
   "outputs": [],
   "source": "ALTER DYNAMIC TABLE INTERMEDIATE_VWAP_STOCK_TRADES_DT_DEMO_DT SUSPEND;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fdc8c778-d578-4410-8661-4c55261ac65f",
   "metadata": {
    "language": "sql",
    "name": "SQLResumeDT"
   },
   "outputs": [],
   "source": "ALTER DYNAMIC TABLE INTERMEDIATE_VWAP_STOCK_TRADES_DT_DEMO_DT RESUME;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae88afbf-eeb4-49d8-8c3a-2500b2c3c08f",
   "metadata": {
    "name": "MDManualRefreshDT",
    "collapsed": false
   },
   "source": "## [Manually refresh a dynamic table](https://docs.snowflake.com/en/user-guide/dynamic-tables-manage#manually-refresh-a-dynamic-table)\n- To get the latest data, manually refresh a dynamic table before its next scheduled refresh. \n- This is useful for large target lags or one-time freshness needs. \n- For example, if a dynamic table is configured with a large target lag and its next refresh is hours away, a manual refresh ensures up-to-date data.\n- Manual refreshes are never skipped but they can cause scheduled refreshes to skip, especially if you perform frequent manual refreshes on a dynamic table.\n- Doing so can prevent downstream dynamic tables from refreshing. \n- For this reason, Snowflake recommends that you avoid frequently performing manual refreshes on a dynamic table with downstream dynamic tables that are expected to refresh according to target lag."
  },
  {
   "cell_type": "code",
   "id": "6891ab05-4cd2-40cc-8e89-dc8b1a3c082a",
   "metadata": {
    "language": "sql",
    "name": "SQLManualRefreshDT",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "ALTER DYNAMIC TABLE INTERMEDIATE_VWAP_STOCK_TRADES_DT_DEMO_DT REFRESH",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5329b946-49d2-4af7-9a2a-04024990d557",
   "metadata": {
    "name": "MDMonitoringDT",
    "collapsed": false
   },
   "source": "# Section 6: Monitoring Dynamic Tables"
  },
  {
   "cell_type": "markdown",
   "id": "9aee0c00-3b0f-41cd-9fc9-95ef0408e6bd",
   "metadata": {
    "name": "MDDTGraphHistory",
    "collapsed": false
   },
   "source": "## Use Dynamic Table Graph History to Check Scheduling State."
  },
  {
   "cell_type": "code",
   "id": "3ea8aa66-2025-49d1-929b-dec4ad54a2b0",
   "metadata": {
    "language": "sql",
    "name": "SQLDTGraphHistory"
   },
   "outputs": [],
   "source": "--\n-- Look for SCHEDULING_STATE to Find out the State of the Dynamic Table.  \n-- INPUT column shows the upstream object dependency.  \n-- \nSELECT *\n  FROM TABLE (INFORMATION_SCHEMA.DYNAMIC_TABLE_GRAPH_HISTORY());",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cce732b8-33df-463f-9e7e-4542362af07c",
   "metadata": {
    "name": "MDSampleDTSchedulingError",
    "collapsed": false
   },
   "source": "## A sample error stored in the scheduling state column\n```json\n{\n  \"reason_code\": \"SUSPENDED_DUE_TO_ERRORS\",\n  \"reason_message\": \"The DT was suspended due to 5 consecutive refresh errors\",\n  \"state\": \"SUSPENDED\",\n  \"suspended_on\": \"2025-04-05 07:11:35.662 -0700\"\n}"
  },
  {
   "cell_type": "markdown",
   "id": "c12878c5-f5ec-4753-bd0a-a6473420bb2a",
   "metadata": {
    "name": "MDCheckingDTRefreshHistory",
    "collapsed": false
   },
   "source": "## [Determine the optimal target lag for a dynamic table](https://docs.snowflake.com/en/user-guide/dynamic-tables-target-lag#determine-the-optimal-target-lag-for-a-dynamic-table)\n### Checking Dynamic Table Refresh History\n- Snowflake schedules refreshes slightly earlier to allow time for the refresh to complete. \n- For example, if you set the target lag to 5 minutes, it doesn’t mean the table will refresh exactly every 5 minutes. \n- Actual refresh intervals might be shorter than the specified lag. \n- If you want more consistent 5-minute refreshes, consider increasing the target lag slightly.\n- You can use either the DYNAMIC_TABLE_REFRESH_HISTORY table function in INFORMATION_SCHEMA or Snowsight to determine the optimal target lag time per your requirements. \n- For example, analyzing refresh details, including duration and skipped refreshes, to make an informed decision.\n- **This table function returns information about each refresh (completed and running) of dynamic tables.**"
  },
  {
   "cell_type": "code",
   "id": "be2d6f9b-9235-409d-bfe9-19c2f2e29550",
   "metadata": {
    "language": "sql",
    "name": "SQLDynamic_Table_Refresh_History"
   },
   "outputs": [],
   "source": "-- Function Called Without Database Name or Name Prefix.  \n-- \n-- SELECT\n--     *\n-- FROM\n--   TABLE (\n--     INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY (\n--     )\n--   )\n-- \n-- NAME_PREFIX => 'DEMODB.EQUITY_RESEARCH.'\n--\n-- SELECT\n--     *\n-- FROM\n--   TABLE (\n--     INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY (\n--         NAME_PREFIX => 'DEMODB.EQUITY_RESEARCH.'\n--     )\n--   )\n-- With NAME of the Dynamic Table\nSELECT\n    *\nFROM\n  TABLE (\n    INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY (\n        NAME => 'DEMODB.EQUITY_RESEARCH.INTERMEDIATE_VWAP_STOCK_TRADES_DT'\n    )\n  )",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "69167836-790b-4e3c-8a94-f6ea3db5da85",
   "metadata": {
    "name": "MDDTRefreshErrors",
    "collapsed": false
   },
   "source": "## Show Dynamic Tables with Refresh Errors"
  },
  {
   "cell_type": "code",
   "id": "52b1f274-5230-4053-b65d-c8929cec05ae",
   "metadata": {
    "language": "sql",
    "name": "SQLDynamic_Table_Refresh_History_Error_Only"
   },
   "outputs": [],
   "source": "-- Show Dynamic Tables with Refresh Errors\nSELECT\n    *\nFROM\n  TABLE (\n    INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY (\n      NAME_PREFIX => 'DEMODB.EQUITY_RESEARCH.', ERROR_ONLY => TRUE\n    )\n  )\n-- WHERE\n--    NAME ILIKE '%VWAP%';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "faf6819c-a4b2-4966-a834-e9a626b0a1ab",
   "metadata": {
    "language": "sql",
    "name": "SQLSelectDT",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "Select * FROM INTERMEDIATE_VWAP_STOCK_TRADES_DT_DEMO_DT IDT, VWAP_STOCK_TRADES_DT_DEMO_DT FDT WHERE IDT.TRADE_TIME_SLICE = FDT.TRADE_TIME_SLICE; ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0f6b1e67-6e9c-4676-b060-d33d0e4b4aff",
   "metadata": {
    "name": "MDEventsTableIntro",
    "collapsed": false
   },
   "source": "## Introduction to [Event Table](https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-setting-up) in Snowflake :snowflake:\nAs your Snowflake objects—including procedures and UDFs—emit telemetry data, Snowflake collects the data in an event table whose data is available for queries. Snowflake includes an event table by default, but you can also create a new one.\nTo collect telemetry data, you must have an active event table and have set telemetry levels to allow data collection. If you **don’t already have an active event table**, Snowflake **makes the default event table the active event table.**\n### [What is an Event Table?](https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-setting-up#what-is-an-event-table)\nAn event table is a special kind of database table with a predefined set of columns. The table’s structure supports the data model for OpenTelemetry, a framework for handling telemetry data. When an event table is active, Snowflake collects telemetry data in the table—including data that Snowflake itself generates and data that you emit by instrumenting your handler code using certain APIs. You can view the collected data by executing SQL queries.\n### [Cost of Telemetry Data Collection](https://docs.snowflake.com/en/developer-guide/logging-tracing/logging-tracing-billing)\nWhen you log messages from a function or procedure, Snowflake collects the messages in batches and ingests the batches into the event table.\n\nTo perform this work, Snowflake uses Snowflake-managed resources, also referred to as the serverless compute model. As is the case with other serverless features, Snowflake bills your account for the compute resource and cloud services usage needed to ingest the logged messages. These costs appear on your bill as separate line items.\n\nTo determine the credit usage for logging over time, use the ```EVENT_USAGE_HISTORY``` view.\n\n### [Event Capture Levels](https://docs.snowflake.com/en/user-guide/dynamic-tables-monitor#set-the-severity-level-of-the-events-to-capture)\n\n```ERROR```: Refresh failure events.\n\n```WARN```: Failures to refresh upstream dynamic tables and refresh failure events.\n\n```INFO```: Successful refresh events, failures to refresh upstream dynamic tables, and refresh failure events.\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "ab28b3d9-36a3-414e-afaf-9cbb1a7d0ce8",
   "metadata": {
    "name": "MDErrorMsgCollectionAllObjects",
    "collapsed": false
   },
   "source": "## Error-level Event Collection for All Objects in the Account"
  },
  {
   "cell_type": "code",
   "id": "539c8495-1271-4b52-adc3-b12432202b81",
   "metadata": {
    "language": "sql",
    "name": "SQLErrorMsgCollectionAllObjects"
   },
   "outputs": [],
   "source": "ALTER ACCOUNT SET LOG_LEVEL = WARN;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b2ff64a0-3514-417d-8bf4-d7584ce757aa",
   "metadata": {
    "name": "MDInfoMsgCollectionDB",
    "collapsed": false
   },
   "source": "## Info-level Event Collection for All Objects in a Database"
  },
  {
   "cell_type": "code",
   "id": "ceee6b08-1337-485e-84e6-7a1064f9b0bd",
   "metadata": {
    "language": "sql",
    "name": "SQLInfoMsgCollectionDB"
   },
   "outputs": [],
   "source": "ALTER DATABASE DEMODB SET LOG_LEVEL = INFO;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e112d608-1c96-4250-950d-2973482f4557",
   "metadata": {
    "name": "MDCaptureWarnLevelEventsForDT",
    "collapsed": false
   },
   "source": "## Warn-level Event Collection for a Dynamic Table"
  },
  {
   "cell_type": "code",
   "id": "5c43255c-9e92-4fea-9f59-4f34b563b18d",
   "metadata": {
    "language": "sql",
    "name": "SQLCaptureWarnLevelEventsForDT"
   },
   "outputs": [],
   "source": "ALTER DYNAMIC TABLE INTERMEDIATE_VWAP_STOCK_TRADES_DT SET LOG_LEVEL = WARN;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9ac14e6b-9bcb-4601-b8ab-3987800d824d",
   "metadata": {
    "name": "MDDisableEventCollection",
    "collapsed": false
   },
   "source": "## Disable the collection of logging and tracing events in the account"
  },
  {
   "cell_type": "code",
   "id": "f1b46a15-43ef-43bb-84d3-7d05f7b28a91",
   "metadata": {
    "language": "sql",
    "name": "SQLDisableEventCollection"
   },
   "outputs": [],
   "source": "ALTER ACCOUNT SET EVENT_TABLE = NONE",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62e9acfd-b27d-42d2-9ad0-aa5ed9880c25",
   "metadata": {
    "language": "sql",
    "name": "SQLSelectEVENTS_VIEW"
   },
   "outputs": [],
   "source": "SELECT * FROM SNOWFLAKE.TELEMETRY.EVENTS_VIEW\n      WHERE resource_attributes:\"snow.executable.type\" = 'dynamic_table'\n        -- AND resource_attributes:\"snow.database.name\" = 'DEMODB'\n        -- AND record_attributes:\"event.name\" = 'refresh.status'\n        -- AND record:\"severity_text\" = 'WARN'\n        -- AND value:\"state\" = 'FAILED'",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3b60eb6-7ec8-4736-84bc-af62e5cc5ad2",
   "metadata": {
    "name": "MDDTEMailNotification",
    "collapsed": false
   },
   "source": "## Set-up E-mail Notification for Monitoring"
  },
  {
   "cell_type": "code",
   "id": "95eb044f-3e92-4f05-a54a-53389ad93bb2",
   "metadata": {
    "language": "sql",
    "name": "SQLCreateNotificationIntegration"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE NOTIFICATION INTEGRATION MY_EMAIL_INTEGRATION\nTYPE = EMAIL\nENABLED = TRUE\nALLOWED_RECIPIENTS = ('prasanna.rajagopal@snowflake.com');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3d276267-c9c9-442e-8500-316cfaa68660",
   "metadata": {
    "name": "MDTestEmailNotification",
    "collapsed": false
   },
   "source": "## Test the E-mail Notification."
  },
  {
   "cell_type": "code",
   "id": "02db7ba4-21fa-4e7c-8bc9-3ac7fc98002d",
   "metadata": {
    "language": "sql",
    "name": "cell36"
   },
   "outputs": [],
   "source": "CALL SYSTEM$SEND_EMAIL(\n    'MY_EMAIL_INTEGRATION',  -- Notification integration name\n    'prasanna.rajagopal@snowflake.com',      -- Recipient email address\n    'Alert: Testing Dynamic Table', -- Email subject\n    'Testing DT' -- Email body\n);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "840fd7d9-873e-4e15-9efc-0f2874d76710",
   "metadata": {
    "language": "sql",
    "name": "SQLCreateEMAILAlert",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE ALERT my_alert_on_dt_refreshes\n  IF( EXISTS(\n    SELECT * FROM SNOWFLAKE.TELEMETRY.EVENTS_VIEW\n      WHERE resource_attributes:\"snow.executable.type\" = 'dynamic_table'\n        AND resource_attributes:\"snow.database.name\" = 'DEMODB'\n        AND record_attributes:\"event.name\" = 'refresh.status'\n        AND record:\"severity_text\" = 'ERROR'\n        AND value:\"state\" = 'FAILED'))\n  THEN\n    BEGIN\n      LET result_str VARCHAR;\n      (SELECT ARRAY_TO_STRING(ARRAY_ARG(name)::ARRAY, ',') INTO :result_str\n         FROM (\n           SELECT resource_attributes:\"snow.executable.name\"::VARCHAR name\n             FROM TABLE(RESULT_SCAN(SNOWFLAKE.ALERT.GET_CONDITION_QUERY_UUID()))\n             LIMIT 10\n         )\n      );\n      CALL SYSTEM$SEND_EMAIL(\n        'MY_EMAIL_INTEGRATION',  -- Notification integration name\n        'prasanna.rajagopal@snowflake.com',      -- Recipient email address\n        name, -- Email subject\n        :result_str -- Email body\n        );\n    END;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "54b3fc57-c699-4b70-a23e-6501565b5ddd",
   "metadata": {
    "name": "MDQueryMonitorRefreshes",
    "collapsed": false
   },
   "source": "## [Query the event table to monitor refreshes](https://docs.snowflake.com/en/user-guide/dynamic-tables-monitor#query-the-event-table-to-monitor-refreshes)\n- Change the database name in the where clause below to match your database name.\n\n```resource_attributes:\"snow.database.name\" = 'DEMODB'```"
  },
  {
   "cell_type": "code",
   "id": "b900b1b2-c4ae-43da-b95b-be9adb3cf7c4",
   "metadata": {
    "language": "sql",
    "name": "SQLQueryMonitorRefreshes",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT\n    timestamp,\n    resource_attributes:\"snow.executable.name\"::VARCHAR AS dt_name,\n    resource_attributes:\"snow.query.id\"::VARCHAR AS query_id,\n    value:message::VARCHAR AS error\n  FROM SNOWFLAKE.TELEMETRY.EVENTS_VIEW\n  WHERE\n    resource_attributes:\"snow.executable.type\" = 'DYNAMIC_TABLE' AND\n    resource_attributes:\"snow.database.name\" = 'DEMODB' AND\n    value:state = 'FAILED'\n  ORDER BY timestamp DESC;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "23339599-35af-49f8-a29c-8014406b73ba",
   "metadata": {
    "name": "MDCostMonitoring",
    "collapsed": false
   },
   "source": "# Section 7: Cost Monitoring"
  },
  {
   "cell_type": "markdown",
   "id": "2ba380a9-d729-49cd-940e-a41830855083",
   "metadata": {
    "name": "MDDTCostUnderstanding",
    "collapsed": false
   },
   "source": "## [Understanding cost for dynamic tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-cost)\n### Dynamic Tables incur compute and storage costs. \n### [Compute Costs](https://docs.snowflake.com/en/user-guide/dynamic-tables-cost#compute-cost)\nThere are two compute costs associated with dynamic tables: \n- **Virtual warehouses**\n- **Cloud Services compute**\n\nDynamic tables require virtual warehouses to refresh - that is, run queries against base objects when they are initialized and refreshed, including both scheduled and manual refreshes. These operations use compute resources, which consume credits.\n\nDynamic tables also require Cloud Services compute to identify changes in underlying base objects and whether the virtual warehouse needs to be invoked. If no changes are identified, virtual warehouse credits aren’t consumed since there’s no new data to refresh. Note that there may be instances where changes in base objects are filtered out in the dynamic table query. In such scenarios, virtual warehouse credits are consumed because the dynamic table undergoes a refresh to determine whether the changes are applicable.\n\n**If the associated virtual warehouse is suspended and no changes in base objects are identified, the suspended virtual warehouse doesn’t get invoked and no credits are consumed.** \n\nConversely, if changes are identified, the virtual warehouse is automatically resumed to process the updates.\n\nDynamic table refreshes are driven by the configured target lag. Dynamic table pipelines with lower target lag refresh more often and therefore incur higher compute costs.\n\n### [Storage Costs](https://docs.snowflake.com/en/user-guide/dynamic-tables-cost#storage-cost)\nDynamic tables require storage to store the materialized results. Similar to regular tables, you may incur additional storage cost for Time Travel, fail-safe storage, and cloning feature.\n### Potential for storage cost savings\n- [Transient Dynamic Tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-cost#transient-dynamic-tables)\n- [Dynamic Iceberg Table](https://docs.snowflake.com/en/user-guide/dynamic-tables-create-iceberg)\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "d8b6b78d-2eed-467e-acf6-27071827e165",
   "metadata": {
    "name": "MDWAREHOUSE_METERING_HISTORY",
    "collapsed": false
   },
   "source": "## WAREHOUSE_METERING_HISTORY\nThis table function can be used in queries to return the **hourly credit usage** for a single warehouse (or all the warehouses in your account) within a specified date range."
  },
  {
   "cell_type": "code",
   "id": "a09f29b9-3767-4426-bfa8-494413db1faf",
   "metadata": {
    "language": "sql",
    "name": "SQLWarehouseMetering",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT\n    warehouse_name,\n    CREDITS_USED,\n    CREDITS_USED_COMPUTE,\n    CREDITS_USED_CLOUD_SERVICES,\n    (credits_used_compute -\n    credits_attributed_compute_queries) AS idle_compute_cost,\n    DIV0(idle_compute_cost, CREDITS_USED_COMPUTE) idle_as_percent_of_compute_credits,\nFROM \n    SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY\nWHERE start_time >= DATEADD('days', -20, CURRENT_DATE())\n  AND end_time < CURRENT_DATE()\nORDER BY WAREHOUSE_NAME;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7fe9da42-c53c-4205-b5d2-e714a8e9f6a7",
   "metadata": {
    "name": "MDQUERY_ATTRIBUTION_HISTORYvw",
    "collapsed": false
   },
   "source": "## [QUERY_ATTRIBUTION_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/query_attribution_history) view\nThe value in the ```credits_attributed_compute``` column contains the warehouse credit usage for executing the query, inclusive of any resizing and/or autoscaling of multi-cluster warehouse(s). This cost is attributed based on the weighted average of the resource consumption.\n\nThe value doesn’t include any credit usage for warehouse idle time. Idle time is a period of time in which no queries are running in the warehouse and can be measured at the warehouse level.\n\nThe value doesn’t include any other credit usage that is incurred as a result of query execution. For example, the following are not included in the query cost:\n\n- Data transfer costs\n\n- Storage costs\n\n- Cloud services costs\n\n- Costs for serverless features\n\n- Costs for tokens processed by AI services"
  },
  {
   "cell_type": "code",
   "id": "3dda8349-68cf-4457-95cd-6b9d1b434599",
   "metadata": {
    "language": "sql",
    "name": "SQLQuery_Attribution_History",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY WHERE WAREHOUSE_NAME = 'VWAP_DT_WH' LIMIT 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42209eab-2f34-4d17-b9d7-61fa8eca3d2b",
   "metadata": {
    "language": "sql",
    "name": "SQLMeteringDailyHistoryView",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT\n    usage_date,\n    credits_used_cloud_services,\n    credits_adjustment_cloud_services,\n    credits_used_cloud_services + credits_adjustment_cloud_services AS billed_cloud_services\nFROM \n    snowflake.account_usage.metering_daily_history\nWHERE\n    SERVICE_TYPE = 'WAREHOUSE_METERING'\nAND\n    usage_date >= DATEADD(month,-1,CURRENT_TIMESTAMP())\nAND \n    credits_used_cloud_services > 0\nORDER BY 4 DESC;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3206066-75b8-4d08-8037-56c1cc54039c",
   "metadata": {
    "name": "MDDynamicTableBestPractices",
    "collapsed": false
   },
   "source": "# Section 8: [Dynamic Table Best Practices](https://docs.snowflake.com/en/user-guide/dynamic-tables-best-practices#best-practices-for-creating-dynamic-tables)\n### 1. [Chain together pipelines of dynamic tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-best-practices#chain-together-pipelines-of-dynamic-tables)\n    - When defining a new dynamic table, rather than defining a large dynamic table with many nested statements, use small dynamic tables with pipelines instead.\n### 2. Use a [“controller” dynamic table](https://docs.snowflake.com/en/user-guide/dynamic-tables-best-practices#use-a-controller-dynamic-table-for-complex-task-graphs) for complex task graphs\n    - When you have a complex graph of dynamic tables with many roots and leaves and you want to perform operations (e.g. changing lag, manual refresh, suspension) on the full task graph with a single command, do the following:\n\n    - Set the value for the TARGET_LAG of all of your dynamic tables to DOWNSTREAM.\n\n    - Create a “controller” dynamic table that reads from all of the leaves in your task graph. To ensure this controller doesn’t consume resources, do the following:\n\n```SQL\nCREATE DYNAMIC TABLE controller\n    TARGET_LAG = <target_lag>\n    WAREHOUSE = <warehouse>\nAS\n    SELECT 1 A FROM <leaf1>, …, <leafN> LIMIT 0;\n```\n- Use the controller to control the whole graph. For example:\n\n- Set a new target lag for the task graph.\n\n```SQL \n    ALTER DYNAMIC TABLE controller SET TARGET_LAG = <new_target_lag>\n```\n- Manually refresh the task graph.\n```SQL\nALTER DYNAMIC TABLE controller REFRESH\n```\n### 3. [Cloning pipelines of dynamic tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-best-practices#about-cloning-pipelines-of-dynamic-tables)\n- Avoid reinitializations of your pipeline by cloning all elements of the dynamic table pipeline in the same clone command to avoid reinitializations of your pipeline. \n- You can do this by **consolidating all elements of the pipeline (e.g. base tables, view, and dynamic tables) in the same schema or database.**\n### 4. [Use transient dynamic tables to reduce storage cost](https://docs.snowflake.com/en/user-guide/dynamic-tables-best-practices#use-transient-dynamic-tables-to-reduce-storage-cost)\n- Transient dynamic tables maintain data reliably over time and support Time Travel within the data retention period, but don’t retain data beyond the fail-safe period. \n- By default, dynamic table data is retained for 7 days in fail-safe storage. For dynamic tables with high refresh throughput, this can significantly increase storage consumption. \n- Therefore, you should make a dynamic table transient only if its data doesn’t need the same level of data protection and recovery provided by permanent tables.\n### 5. Use dedicated warehouses for refreshes\n### 6. [Use downstream lag](https://docs.snowflake.com/en/user-guide/dynamic-tables-best-practices#use-downstream-lag)\n- Downstream lag indicates that the dynamic table should refresh when other dependent dynamic tables require refreshing. \n- You should use downstream lag as a best practice because of its ease of use and cost effectiveness. \n- Without downstream lag, **managing a chain of complex dynamic tables would require individually assigning each table its own target lag** and managing the associated constraints, instead of only monitoring the data freshness of the final table. \n### 7. [Set the refresh mode for all production dynamic tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-best-practices#set-the-refresh-mode-for-all-production-dynamic-tables)\n- ```AUTO```\n    - The system attempts to apply incremental refresh by default. \n    - When incremental refresh isn’t supported or might not perform well, the dynamic table automatically selects full refresh instead.\n- ```INCREMENTAL```\n- ```FULL```"
  }
 ]
}